  branch-instructions OR branches                    [Hardware event]
  branch-misses                                      [Hardware event]
  bus-cycles                                         [Hardware event]
  cache-misses                                       [Hardware event]
  cache-references                                   [Hardware event]
  cpu-cycles OR cycles                               [Hardware event]
  instructions                                       [Hardware event]
  ref-cycles                                         [Hardware event]
  alignment-faults                                   [Software event]
  bpf-output                                         [Software event]
  cgroup-switches                                    [Software event]
  context-switches OR cs                             [Software event]
  cpu-clock                                          [Software event]
  cpu-migrations OR migrations                       [Software event]
  dummy                                              [Software event]
  emulation-faults                                   [Software event]
  major-faults                                       [Software event]
  minor-faults                                       [Software event]
  page-faults OR faults                              [Software event]
  task-clock                                         [Software event]
  duration_time                                      [Tool event]
  user_time                                          [Tool event]
  system_time                                        [Tool event]

cpu:
  L1-dcache-loads OR cpu/L1-dcache-loads/
  L1-dcache-load-misses OR cpu/L1-dcache-load-misses/
  L1-dcache-stores OR cpu/L1-dcache-stores/
  L1-icache-load-misses OR cpu/L1-icache-load-misses/
  LLC-loads OR cpu/LLC-loads/
  LLC-load-misses OR cpu/LLC-load-misses/
  LLC-stores OR cpu/LLC-stores/
  LLC-store-misses OR cpu/LLC-store-misses/
  dTLB-loads OR cpu/dTLB-loads/
  dTLB-load-misses OR cpu/dTLB-load-misses/
  dTLB-stores OR cpu/dTLB-stores/
  dTLB-store-misses OR cpu/dTLB-store-misses/
  iTLB-load-misses OR cpu/iTLB-load-misses/
  branch-loads OR cpu/branch-loads/
  branch-load-misses OR cpu/branch-load-misses/
  node-loads OR cpu/node-loads/
  node-load-misses OR cpu/node-load-misses/
  node-stores OR cpu/node-stores/
  node-store-misses OR cpu/node-store-misses/
  branch-instructions OR cpu/branch-instructions/    [Kernel PMU event]
  branch-misses OR cpu/branch-misses/                [Kernel PMU event]
  bus-cycles OR cpu/bus-cycles/                      [Kernel PMU event]
  cache-misses OR cpu/cache-misses/                  [Kernel PMU event]
  cache-references OR cpu/cache-references/          [Kernel PMU event]
  cpu-cycles OR cpu/cpu-cycles/                      [Kernel PMU event]
  instructions OR cpu/instructions/                  [Kernel PMU event]
  mem-loads OR cpu/mem-loads/                        [Kernel PMU event]
  mem-stores OR cpu/mem-stores/                      [Kernel PMU event]
  ref-cycles OR cpu/ref-cycles/                      [Kernel PMU event]
  slots OR cpu/slots/                                [Kernel PMU event]
  topdown-bad-spec OR cpu/topdown-bad-spec/          [Kernel PMU event]
  topdown-be-bound OR cpu/topdown-be-bound/          [Kernel PMU event]
  topdown-fe-bound OR cpu/topdown-fe-bound/          [Kernel PMU event]
  topdown-retiring OR cpu/topdown-retiring/          [Kernel PMU event]
  cstate_core/c6-residency/                          [Kernel PMU event]
  cstate_core/c7-residency/                          [Kernel PMU event]
  cstate_pkg/c10-residency/                          [Kernel PMU event]
  cstate_pkg/c2-residency/                           [Kernel PMU event]
  cstate_pkg/c3-residency/                           [Kernel PMU event]
  cstate_pkg/c6-residency/                           [Kernel PMU event]
  cstate_pkg/c7-residency/                           [Kernel PMU event]
  cstate_pkg/c8-residency/                           [Kernel PMU event]
  cstate_pkg/c9-residency/                           [Kernel PMU event]
  i915/actual-frequency/                             [Kernel PMU event]
  i915/bcs0-busy/                                    [Kernel PMU event]
  i915/bcs0-sema/                                    [Kernel PMU event]
  i915/bcs0-wait/                                    [Kernel PMU event]
  i915/interrupts/                                   [Kernel PMU event]
  i915/rc6-residency/                                [Kernel PMU event]
  i915/rcs0-busy/                                    [Kernel PMU event]
  i915/rcs0-sema/                                    [Kernel PMU event]
  i915/rcs0-wait/                                    [Kernel PMU event]
  i915/requested-frequency/                          [Kernel PMU event]
  i915/software-gt-awake-time/                       [Kernel PMU event]
  i915/vcs0-busy/                                    [Kernel PMU event]
  i915/vcs0-sema/                                    [Kernel PMU event]
  i915/vcs0-wait/                                    [Kernel PMU event]
  i915/vcs1-busy/                                    [Kernel PMU event]
  i915/vcs1-sema/                                    [Kernel PMU event]
  i915/vcs1-wait/                                    [Kernel PMU event]
  i915/vecs0-busy/                                   [Kernel PMU event]
  i915/vecs0-sema/                                   [Kernel PMU event]
  i915/vecs0-wait/                                   [Kernel PMU event]
  intel_bts//                                        [Kernel PMU event]
  intel_pt//                                         [Kernel PMU event]
  msr/aperf/                                         [Kernel PMU event]
  msr/cpu_thermal_margin/                            [Kernel PMU event]
  msr/mperf/                                         [Kernel PMU event]
  msr/pperf/                                         [Kernel PMU event]
  msr/smi/                                           [Kernel PMU event]
  msr/tsc/                                           [Kernel PMU event]
  power/energy-cores/                                [Kernel PMU event]
  power/energy-gpu/                                  [Kernel PMU event]
  power/energy-pkg/                                  [Kernel PMU event]
  power/energy-psys/                                 [Kernel PMU event]
  uncore_clock/clockticks/                           [Kernel PMU event]
  uncore_imc_free_running/data_read/                 [Kernel PMU event]
  uncore_imc_free_running/data_total/                [Kernel PMU event]
  uncore_imc_free_running/data_write/                [Kernel PMU event]

cache:
  l1d.replacement
       [Counts the number of cache lines replaced in L1 data cache. Unit: cpu]
  l1d_pend_miss.fb_full
       [Number of cycles a demand request has waited due to L1D Fill Buffer
        (FB) unavailability. Unit: cpu]
  l1d_pend_miss.fb_full_periods
       [Number of phases a demand request has waited due to L1D Fill Buffer
        (FB) unavailability. Unit: cpu]
  l1d_pend_miss.l2_stall
       [Number of cycles a demand request has waited due to L1D due to lack of
        L2 resources. Unit: cpu]
  l1d_pend_miss.pending
       [Number of L1D misses that are outstanding. Unit: cpu]
  l1d_pend_miss.pending_cycles
       [Cycles with L1D load Misses outstanding. Unit: cpu]
  l2_lines_in.all
       [L2 cache lines filling L2. Unit: cpu]
  l2_lines_out.non_silent
       [Modified cache lines that are evicted by L2 cache when triggered by an
        L2 cache fill. Unit: cpu]
  l2_lines_out.silent
       [Non-modified cache lines that are silently dropped by L2 cache when
        triggered by an L2 cache fill. Unit: cpu]
  l2_rqsts.all_code_rd
       [L2 code requests. Unit: cpu]
  l2_rqsts.all_demand_data_rd
       [Demand Data Read access L2 cache. Unit: cpu]
  l2_rqsts.all_rfo
       [RFO requests to L2 cache. Unit: cpu]
  l2_rqsts.code_rd_hit
       [L2 cache hits when fetching instructions,code reads. Unit: cpu]
  l2_rqsts.code_rd_miss
       [L2 cache misses when fetching instructions. Unit: cpu]
  l2_rqsts.demand_data_rd_hit
       [Demand Data Read requests that hit L2 cache. Unit: cpu]
  l2_rqsts.demand_data_rd_miss
       [Demand Data Read miss L2 cache. Unit: cpu]
  l2_rqsts.miss
       [Read requests with true-miss in L2 cache. Unit: cpu]
  l2_rqsts.references
       [All accesses to L2 cache. Unit: cpu]
  l2_rqsts.rfo_hit
       [RFO requests that hit L2 cache. Unit: cpu]
  l2_rqsts.rfo_miss
       [RFO requests that miss L2 cache. Unit: cpu]
  l2_rqsts.swpf_hit
       [SW prefetch requests that hit L2 cache. Unit: cpu]
  l2_rqsts.swpf_miss
       [SW prefetch requests that miss L2 cache. Unit: cpu]
  l2_trans.l2_wb
       [L2 writebacks that access L2 cache. Unit: cpu]
  lock_cycles.cache_lock_duration
       [Cycles when L1D is locked. Unit: cpu]
  longest_lat_cache.miss
       [Core-originated cacheable requests that missed L3 (Except hardware
        prefetches to the L3). Unit: cpu]
  mem_inst_retired.all_loads
       [Retired load instructions Supports address when precise (Precise
        event). Unit: cpu]
  mem_inst_retired.all_stores
       [Retired store instructions Supports address when precise (Precise
        event). Unit: cpu]
  mem_inst_retired.any
       [All retired memory instructions Supports address when precise (Precise
        event). Unit: cpu]
  mem_inst_retired.lock_loads
       [Retired load instructions with locked access Supports address when
        precise (Precise event). Unit: cpu]
  mem_inst_retired.split_loads
       [Retired load instructions that split across a cacheline boundary
        Supports address when precise (Precise event). Unit: cpu]
  mem_inst_retired.split_stores
       [Retired store instructions that split across a cacheline boundary
        Supports address when precise (Precise event). Unit: cpu]
  mem_inst_retired.stlb_miss_loads
       [Retired load instructions that miss the STLB Supports address when
        precise (Precise event). Unit: cpu]
  mem_inst_retired.stlb_miss_stores
       [Retired store instructions that miss the STLB Supports address when
        precise (Precise event). Unit: cpu]
  mem_load_l3_hit_retired.xsnp_fwd
       [Snoop hit a modified(HITM) or clean line(HIT_W_FWD) in another on-pkg
        core which forwarded the data back due to a retired load instruction
        Supports address when precise (Precise event). Unit: cpu]
  mem_load_l3_hit_retired.xsnp_miss
       [Retired load instructions whose data sources were L3 hit and
        cross-core snoop missed in on-pkg core cache Supports address when
        precise (Precise event). Unit: cpu]
  mem_load_l3_hit_retired.xsnp_no_fwd
       [Snoop hit without forwarding in another on-pkg core due to a retired
        load instruction,data was supplied by the L3 Supports address when
        precise (Precise event). Unit: cpu]
  mem_load_l3_hit_retired.xsnp_none
       [Retired load instructions whose data sources were hits in L3 without
        snoops required Supports address when precise (Precise event). Unit:
        cpu]
  mem_load_misc_retired.uc
       [Retired instructions with at least 1 uncacheable load or lock Supports
        address when precise (Precise event). Unit: cpu]
  mem_load_retired.fb_hit
       [Number of completed demand load requests that missed the L1,but hit
        the FB(fill buffer),because a preceding miss to the same cacheline
        initiated the line to be brought into L1,but data is not yet ready in
        L1 Supports address when precise (Precise event). Unit: cpu]
  mem_load_retired.l1_hit
       [Retired load instructions with L1 cache hits as data sources Supports
        address when precise (Precise event). Unit: cpu]
  mem_load_retired.l1_miss
       [Retired load instructions missed L1 cache as data sources Supports
        address when precise (Precise event). Unit: cpu]
  mem_load_retired.l2_hit
       [Retired load instructions with L2 cache hits as data sources Supports
        address when precise (Precise event). Unit: cpu]
  mem_load_retired.l2_miss
       [Retired load instructions missed L2 cache as data sources Supports
        address when precise (Precise event). Unit: cpu]
  mem_load_retired.l3_hit
       [Retired load instructions with L3 cache hits as data sources Supports
        address when precise (Precise event). Unit: cpu]
  mem_load_retired.l3_miss
       [Retired load instructions missed L3 cache as data sources Supports
        address when precise (Precise event). Unit: cpu]
  ocr.demand_data_rd.l3_hit.snoop_hit_with_fwd
       [OCR.DEMAND_DATA_RD.L3_HIT.SNOOP_HIT_WITH_FWD. Unit: cpu]
  ocr.demand_data_rd.l3_hit.snoop_hitm
       [Counts demand data reads that hit a cacheline in the L3 where a snoop
        hit in another cores caches,data forwarding is required as the data is
        modified. Unit: cpu]
  ocr.demand_rfo.l3_hit.snoop_hitm
       [Counts demand reads for ownership (RFO) requests and software
        prefetches for exclusive ownership (PREFETCHW) that hit a cacheline in
        the L3 where a snoop hit in another cores caches,data forwarding is
        required as the data is modified. Unit: cpu]
  offcore_requests.all_data_rd
       [Demand and prefetch data reads. Unit: cpu]
  offcore_requests.all_requests
       [Any memory transaction that reached the SQ. Unit: cpu]
  offcore_requests.demand_data_rd
       [Demand Data Read requests sent to uncore. Unit: cpu]
  offcore_requests.demand_rfo
       [Demand RFO requests including regular RFOs,locks,ItoM. Unit: cpu]
  offcore_requests_outstanding.all_data_rd
       [Offcore outstanding cacheable Core Data Read transactions in
        SuperQueue (SQ),queue to uncore. Unit: cpu]
  offcore_requests_outstanding.cycles_with_data_rd
       [Cycles when offcore outstanding cacheable Core Data Read transactions
        are present in SuperQueue (SQ),queue to uncore. Unit: cpu]
  offcore_requests_outstanding.cycles_with_demand_data_rd
       [Cycles when offcore outstanding Demand Data Read transactions are
        present in SuperQueue (SQ),queue to uncore. Unit: cpu]
  offcore_requests_outstanding.cycles_with_demand_rfo
       [Cycles with offcore outstanding demand rfo reads transactions in
        SuperQueue (SQ),queue to uncore. Unit: cpu]
  offcore_requests_outstanding.demand_data_rd
       [Demand Data Read transactions pending for off-core. Highly correlated.
        Unit: cpu]
  offcore_requests_outstanding.demand_data_rd_ge_6
       [Cycles with at least 6 offcore outstanding Demand Data Read
        transactions in uncore queue. Unit: cpu]
  offcore_requests_outstanding.demand_rfo
       [Store Read transactions pending for off-core. Highly correlated. Unit:
        cpu]
  sq_misc.bus_lock
       [Counts bus locks,accounts for cache line split locks and UC locks.
        Unit: cpu]
  sq_misc.sq_full
       [Cycles the superQ cannot take any more entries. Unit: cpu]
  sw_prefetch_access.any
       [Counts the number of PREFETCHNTA,PREFETCHW,PREFETCHT0,PREFETCHT1 or
        PREFETCHT2 instructions executed. Unit: cpu]
  sw_prefetch_access.nta
       [Number of PREFETCHNTA instructions executed. Unit: cpu]
  sw_prefetch_access.prefetchw
       [Number of PREFETCHW instructions executed. Unit: cpu]
  sw_prefetch_access.t0
       [Number of PREFETCHT0 instructions executed. Unit: cpu]
  sw_prefetch_access.t1_t2
       [Number of PREFETCHT1 or PREFETCHT2 instructions executed. Unit: cpu]

floating point:
  assists.fp
       [Counts all microcode FP assists. Unit: cpu]
  fp_arith_inst_retired.128b_packed_double
       [Counts number of SSE/AVX computational 128-bit packed double precision
        floating-point instructions retired; some instructions will count
        twice as noted below. Each count represents 2 computation operations,
        one for each element. Applies to SSE* and AVX* packed double precision
        floating-point instructions: ADD SUB HADD HSUB SUBADD MUL DIV MIN MAX
        SQRT DPP FM(N)ADD/SUB. DPP and FM(N)ADD/SUB instructions count twice
        as they perform 2 calculations per element. Unit: cpu]
  fp_arith_inst_retired.128b_packed_single
       [Number of SSE/AVX computational 128-bit packed single precision
        floating-point instructions retired; some instructions will count
        twice as noted below. Each count represents 4 computation operations,
        one for each element. Applies to SSE* and AVX* packed single precision
        floating-point instructions: ADD SUB MUL DIV MIN MAX RCP14 RSQRT14
        SQRT DPP FM(N)ADD/SUB. DPP and FM(N)ADD/SUB instructions count twice
        as they perform 2 calculations per element. Unit: cpu]
  fp_arith_inst_retired.256b_packed_double
       [Counts number of SSE/AVX computational 256-bit packed double precision
        floating-point instructions retired; some instructions will count
        twice as noted below. Each count represents 4 computation operations,
        one for each element. Applies to SSE* and AVX* packed double precision
        floating-point instructions: ADD SUB HADD HSUB SUBADD MUL DIV MIN MAX
        SQRT FM(N)ADD/SUB. FM(N)ADD/SUB instructions count twice as they
        perform 2 calculations per element. Unit: cpu]
  fp_arith_inst_retired.256b_packed_single
       [Counts number of SSE/AVX computational 256-bit packed single precision
        floating-point instructions retired; some instructions will count
        twice as noted below. Each count represents 8 computation operations,
        one for each element. Applies to SSE* and AVX* packed single precision
        floating-point instructions: ADD SUB HADD HSUB SUBADD MUL DIV MIN MAX
        SQRT RSQRT RCP DPP FM(N)ADD/SUB. DPP and FM(N)ADD/SUB instructions
        count twice as they perform 2 calculations per element. Unit: cpu]
  fp_arith_inst_retired.4_flops
       [Number of SSE/AVX computational 128-bit packed single and 256-bit
        packed double precision FP instructions retired; some instructions
        will count twice as noted below. Each count represents 2 or/and 4
        computation operations,1 for each element. Applies to SSE* and AVX*
        packed single precision and packed double precision FP instructions:
        ADD SUB HADD HSUB SUBADD MUL DIV MIN MAX RCP14 RSQRT14 SQRT DPP
        FM(N)ADD/SUB. DPP and FM(N)ADD/SUB count twice as they perform 2
        calculations per element. Unit: cpu]
  fp_arith_inst_retired.512b_packed_double
       [Counts number of SSE/AVX computational 512-bit packed double precision
        floating-point instructions retired; some instructions will count
        twice as noted below. Each count represents 8 computation operations,
        one for each element. Applies to SSE* and AVX* packed double precision
        floating-point instructions: ADD SUB MUL DIV MIN MAX SQRT RSQRT14
        RCP14 FM(N)ADD/SUB. FM(N)ADD/SUB instructions count twice as they
        perform 2 calculations per element. Unit: cpu]
  fp_arith_inst_retired.512b_packed_single
       [Counts number of SSE/AVX computational 512-bit packed single precision
        floating-point instructions retired; some instructions will count
        twice as noted below. Each count represents 16 computation operations,
        one for each element. Applies to SSE* and AVX* packed single precision
        floating-point instructions: ADD SUB MUL DIV MIN MAX SQRT RSQRT14
        RCP14 FM(N)ADD/SUB. FM(N)ADD/SUB instructions count twice as they
        perform 2 calculations per element. Unit: cpu]
  fp_arith_inst_retired.8_flops
       [Number of SSE/AVX computational 256-bit packed single precision and
        512-bit packed double precision FP instructions retired; some
        instructions will count twice as noted below. Each count represents 8
        computation operations,1 for each element. Applies to SSE* and AVX*
        packed single precision and double precision FP instructions: ADD SUB
        HADD HSUB SUBADD MUL DIV MIN MAX SQRT RSQRT RSQRT14 RCP RCP14 DPP
        FM(N)ADD/SUB. DPP and FM(N)ADD/SUB count twice as they perform 2
        calculations per element. Unit: cpu]
  fp_arith_inst_retired.scalar
       [Number of SSE/AVX computational scalar floating-point instructions
        retired; some instructions will count twice as noted below. Applies to
        SSE* and AVX* scalar,double and single precision floating-point: ADD
        SUB MUL DIV MIN MAX RCP14 RSQRT14 RANGE SQRT DPP FM(N)ADD/SUB. DPP and
        FM(N)ADD/SUB instructions count twice as they perform multiple
        calculations per element. Unit: cpu]
  fp_arith_inst_retired.scalar_double
       [Counts number of SSE/AVX computational scalar double precision
        floating-point instructions retired; some instructions will count
        twice as noted below. Each count represents 1 computational operation.
        Applies to SSE* and AVX* scalar double precision floating-point
        instructions: ADD SUB MUL DIV MIN MAX SQRT FM(N)ADD/SUB. FM(N)ADD/SUB
        instructions count twice as they perform 2 calculations per element.
        Unit: cpu]
  fp_arith_inst_retired.scalar_single
       [Counts number of SSE/AVX computational scalar single precision
        floating-point instructions retired; some instructions will count
        twice as noted below. Each count represents 1 computational operation.
        Applies to SSE* and AVX* scalar single precision floating-point
        instructions: ADD SUB MUL DIV MIN MAX SQRT RSQRT RCP FM(N)ADD/SUB.
        FM(N)ADD/SUB instructions count twice as they perform 2 calculations
        per element. Unit: cpu]
  fp_arith_inst_retired.vector
       [Number of any Vector retired FP arithmetic instructions. Unit: cpu]

frontend:
  baclears.any
       [Counts the total number when the front end is resteered,mainly when
        the BPU cannot provide a correct prediction and this is corrected by
        other branch handling mechanisms at the front end. Unit: cpu]
  decode.lcp
       [Stalls caused by changing prefix length of the instruction. [This
        event is alias to ILD_STALL.LCP]. Unit: cpu]
  dsb2mite_switches.count
       [Decode Stream Buffer (DSB)-to-MITE transitions count. Unit: cpu]
  dsb2mite_switches.penalty_cycles
       [DSB-to-MITE switch true penalty cycles. Unit: cpu]
  frontend_retired.any_dsb_miss
       [Retired Instructions who experienced DSB miss (Precise event). Unit:
        cpu]
  frontend_retired.dsb_miss
       [Retired Instructions who experienced a critical DSB miss (Precise
        event). Unit: cpu]
  frontend_retired.itlb_miss
       [Retired Instructions who experienced iTLB true miss (Precise event).
        Unit: cpu]
  frontend_retired.l1i_miss
       [Retired Instructions who experienced Instruction L1 Cache true miss
        (Precise event). Unit: cpu]
  frontend_retired.l2_miss
       [Retired Instructions who experienced Instruction L2 Cache true miss
        (Precise event). Unit: cpu]
  frontend_retired.latency_ge_1
       [Retired instructions after front-end starvation of at least 1 cycle
        (Precise event). Unit: cpu]
  frontend_retired.latency_ge_128
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 128 cycles which was not
        interrupted by a back-end stall (Precise event). Unit: cpu]
  frontend_retired.latency_ge_16
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 16 cycles which was not
        interrupted by a back-end stall (Precise event). Unit: cpu]
  frontend_retired.latency_ge_2
       [Retired instructions after front-end starvation of at least 2 cycles
        (Precise event). Unit: cpu]
  frontend_retired.latency_ge_256
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 256 cycles which was not
        interrupted by a back-end stall (Precise event). Unit: cpu]
  frontend_retired.latency_ge_2_bubbles_ge_1
       [Retired instructions that are fetched after an interval where the
        front-end had at least 1 bubble-slot for a period of 2 cycles which
        was not interrupted by a back-end stall (Precise event). Unit: cpu]
  frontend_retired.latency_ge_32
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 32 cycles which was not
        interrupted by a back-end stall (Precise event). Unit: cpu]
  frontend_retired.latency_ge_4
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 4 cycles which was not
        interrupted by a back-end stall (Precise event). Unit: cpu]
  frontend_retired.latency_ge_512
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 512 cycles which was not
        interrupted by a back-end stall (Precise event). Unit: cpu]
  frontend_retired.latency_ge_64
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 64 cycles which was not
        interrupted by a back-end stall (Precise event). Unit: cpu]
  frontend_retired.latency_ge_8
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 8 cycles which was not
        interrupted by a back-end stall (Precise event). Unit: cpu]
  frontend_retired.stlb_miss
       [Retired Instructions who experienced STLB (2nd level TLB) true miss
        (Precise event). Unit: cpu]
  icache_16b.ifdata_stall
       [Cycles where a code fetch is stalled due to L1 instruction cache miss.
        [This event is alias to ICACHE_DATA.STALLS]. Unit: cpu]
  icache_64b.iftag_hit
       [Instruction fetch tag lookups that hit in the instruction cache (L1I).
        Counts at 64-byte cache-line granularity. Unit: cpu]
  icache_64b.iftag_miss
       [Instruction fetch tag lookups that miss in the instruction cache
        (L1I). Counts at 64-byte cache-line granularity. Unit: cpu]
  icache_64b.iftag_stall
       [Cycles where a code fetch is stalled due to L1 instruction cache tag
        miss. [This event is alias to ICACHE_TAG.STALLS]. Unit: cpu]
  icache_data.stalls
       [Cycles where a code fetch is stalled due to L1 instruction cache miss.
        [This event is alias to ICACHE_16B.IFDATA_STALL]. Unit: cpu]
  icache_tag.stalls
       [Cycles where a code fetch is stalled due to L1 instruction cache tag
        miss. [This event is alias to ICACHE_64B.IFTAG_STALL]. Unit: cpu]
  idq.dsb_cycles_any
       [Cycles Decode Stream Buffer (DSB) is delivering any Uop. Unit: cpu]
  idq.dsb_cycles_ok
       [Cycles DSB is delivering optimal number of Uops. Unit: cpu]
  idq.dsb_uops
       [Uops delivered to Instruction Decode Queue (IDQ) from the Decode
        Stream Buffer (DSB) path. Unit: cpu]
  idq.mite_cycles_any
       [Cycles MITE is delivering any Uop. Unit: cpu]
  idq.mite_cycles_ok
       [Cycles MITE is delivering optimal number of Uops. Unit: cpu]
  idq.mite_uops
       [Uops delivered to Instruction Decode Queue (IDQ) from MITE path. Unit:
        cpu]
  idq.ms_cycles_any
       [Cycles when uops are being delivered to IDQ while MS is busy. Unit:
        cpu]
  idq.ms_switches
       [Number of switches from DSB or MITE to the MS. Unit: cpu]
  idq.ms_uops
       [Uops delivered to IDQ while MS is busy. Unit: cpu]
  idq_uops_not_delivered.core
       [Uops not delivered by IDQ when backend of the machine is not stalled.
        Unit: cpu]
  idq_uops_not_delivered.cycles_0_uops_deliv.core
       [Cycles when no uops are not delivered by the IDQ when backend of the
        machine is not stalled. Unit: cpu]
  idq_uops_not_delivered.cycles_fe_was_ok
       [Cycles when optimal number of uops was delivered to the back-end when
        the back-end is not stalled. Unit: cpu]

memory:
  cycle_activity.stalls_l3_miss
       [Execution stalls while L3 cache miss demand load is outstanding. Unit:
        cpu]
  machine_clears.memory_ordering
       [Number of machine clears due to memory ordering conflicts. Unit: cpu]
  mem_trans_retired.load_latency_gt_128
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 128 cycles Supports address when precise
        (Must be precise). Unit: cpu]
  mem_trans_retired.load_latency_gt_16
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 16 cycles Supports address when precise
        (Must be precise). Unit: cpu]
  mem_trans_retired.load_latency_gt_256
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 256 cycles Supports address when precise
        (Must be precise). Unit: cpu]
  mem_trans_retired.load_latency_gt_32
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 32 cycles Supports address when precise
        (Must be precise). Unit: cpu]
  mem_trans_retired.load_latency_gt_4
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 4 cycles Supports address when precise
        (Must be precise). Unit: cpu]
  mem_trans_retired.load_latency_gt_512
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 512 cycles Supports address when precise
        (Must be precise). Unit: cpu]
  mem_trans_retired.load_latency_gt_64
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 64 cycles Supports address when precise
        (Must be precise). Unit: cpu]
  mem_trans_retired.load_latency_gt_8
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 8 cycles Supports address when precise
        (Must be precise). Unit: cpu]
  offcore_requests.l3_miss_demand_data_rd
       [Demand Data Read requests who miss L3 cache. Unit: cpu]
  rtm_retired.aborted
       [Number of times an RTM execution aborted (Precise event). Unit: cpu]
  rtm_retired.aborted_events
       [Number of times an RTM execution aborted due to none of the previous 4
        categories (e.g. interrupt). Unit: cpu]
  rtm_retired.aborted_mem
       [Number of times an RTM execution aborted due to various memory events
        (e.g. read/write capacity and conflicts). Unit: cpu]
  rtm_retired.aborted_memtype
       [Number of times an RTM execution aborted due to incompatible memory
        type. Unit: cpu]
  rtm_retired.aborted_unfriendly
       [Number of times an RTM execution aborted due to HLE-unfriendly
        instructions. Unit: cpu]
  rtm_retired.commit
       [Number of times an RTM execution successfully committed. Unit: cpu]
  rtm_retired.start
       [Number of times an RTM execution started. Unit: cpu]
  tx_exec.misc2
       [Counts the number of times a class of instructions that may cause a
        transactional abort was executed inside a transactional region. Unit:
        cpu]
  tx_exec.misc3
       [Number of times an instruction execution caused the transactional nest
        count supported to be exceeded. Unit: cpu]
  tx_mem.abort_capacity_read
       [Speculatively counts the number of TSX aborts due to a data capacity
        limitation for transactional reads. Unit: cpu]
  tx_mem.abort_capacity_write
       [Speculatively counts the number of TSX aborts due to a data capacity
        limitation for transactional writes. Unit: cpu]
  tx_mem.abort_conflict
       [Number of times a transactional abort was signaled due to a data
        conflict on a transactionally accessed address. Unit: cpu]

other:
  core_power.lvl0_turbo_license
       [Core cycles where the core was running in a manner where Turbo may be
        clipped to the Non-AVX turbo schedule. Unit: cpu]
  core_power.lvl1_turbo_license
       [Core cycles where the core was running in a manner where Turbo may be
        clipped to the AVX2 turbo schedule. Unit: cpu]
  core_power.lvl2_turbo_license
       [Core cycles where the core was running in a manner where Turbo may be
        clipped to the AVX512 turbo schedule. Unit: cpu]
  ocr.streaming_wr.any_response
       [Counts streaming stores that have any type of response. Unit: cpu]

pipeline:
  arith.divider_active
       [Cycles when divide unit is busy executing divide or square root
        operations. Unit: cpu]
  assists.any
       [Number of occurrences where a microcode assist is invoked by hardware.
        Unit: cpu]
  br_inst_retired.all_branches
       [All branch instructions retired (Precise event). Unit: cpu]
  br_inst_retired.cond
       [Conditional branch instructions retired (Precise event). Unit: cpu]
  br_inst_retired.cond_ntaken
       [Not taken branch instructions retired (Precise event). Unit: cpu]
  br_inst_retired.cond_taken
       [Taken conditional branch instructions retired (Precise event). Unit:
        cpu]
  br_inst_retired.far_branch
       [Far branch instructions retired (Precise event). Unit: cpu]
  br_inst_retired.indirect
       [Indirect near branch instructions retired (excluding returns) (Precise
        event). Unit: cpu]
  br_inst_retired.near_call
       [Direct and indirect near call instructions retired (Precise event).
        Unit: cpu]
  br_inst_retired.near_return
       [Return instructions retired (Precise event). Unit: cpu]
  br_inst_retired.near_taken
       [Taken branch instructions retired (Precise event). Unit: cpu]
  br_misp_retired.all_branches
       [All mispredicted branch instructions retired (Precise event). Unit:
        cpu]
  br_misp_retired.cond
       [Mispredicted conditional branch instructions retired (Precise event).
        Unit: cpu]
  br_misp_retired.cond_ntaken
       [Mispredicted non-taken conditional branch instructions retired
        (Precise event). Unit: cpu]
  br_misp_retired.cond_taken
       [number of branch instructions retired that were mispredicted and taken
        (Precise event). Unit: cpu]
  br_misp_retired.indirect
       [All miss-predicted indirect branch instructions retired (excluding
        RETs. TSX aborts is considered indirect branch) (Precise event). Unit:
        cpu]
  br_misp_retired.indirect_call
       [Mispredicted indirect CALL instructions retired (Precise event). Unit:
        cpu]
  br_misp_retired.near_taken
       [Number of near branch instructions retired that were mispredicted and
        taken (Precise event). Unit: cpu]
  br_misp_retired.ret
       [This event counts the number of mispredicted ret instructions retired.
        Non PEBS (Precise event). Unit: cpu]
  cpu_clk_unhalted.distributed
       [Cycle counts are evenly distributed between active threads in the
        Core. Unit: cpu]
  cpu_clk_unhalted.one_thread_active
       [Core crystal clock cycles when this thread is unhalted and the other
        thread is halted. Unit: cpu]
  cpu_clk_unhalted.ref_distributed
       [Core crystal clock cycles. Cycle counts are evenly distributed between
        active threads in the Core. Unit: cpu]
  cpu_clk_unhalted.ref_tsc
       [Reference cycles when the core is not in halt state. Unit: cpu]
  cpu_clk_unhalted.ref_xclk
       [Core crystal clock cycles when the thread is unhalted. Unit: cpu]
  cpu_clk_unhalted.thread
       [Core cycles when the thread is not in halt state. Unit: cpu]
  cpu_clk_unhalted.thread_p
       [Thread cycles when thread is not in halt state. Unit: cpu]
  cycle_activity.cycles_l1d_miss
       [Cycles while L1 cache miss demand load is outstanding. Unit: cpu]
  cycle_activity.cycles_l2_miss
       [Cycles while L2 cache miss demand load is outstanding. Unit: cpu]
  cycle_activity.cycles_mem_any
       [Cycles while memory subsystem has an outstanding load. Unit: cpu]
  cycle_activity.stalls_l1d_miss
       [Execution stalls while L1 cache miss demand load is outstanding. Unit:
        cpu]
  cycle_activity.stalls_l2_miss
       [Execution stalls while L2 cache miss demand load is outstanding. Unit:
        cpu]
  cycle_activity.stalls_mem_any
       [Execution stalls while memory subsystem has an outstanding load. Unit:
        cpu]
  cycle_activity.stalls_total
       [Total execution stalls. Unit: cpu]
  exe_activity.1_ports_util
       [Cycles total of 1 uop is executed on all ports and Reservation Station
        was not empty. Unit: cpu]
  exe_activity.2_ports_util
       [Cycles total of 2 uops are executed on all ports and Reservation
        Station was not empty. Unit: cpu]
  exe_activity.3_ports_util
       [Cycles total of 3 uops are executed on all ports and Reservation
        Station was not empty. Unit: cpu]
  exe_activity.4_ports_util
       [Cycles total of 4 uops are executed on all ports and Reservation
        Station was not empty. Unit: cpu]
  exe_activity.bound_on_loads
       [Cycles when the memory subsystem has an outstanding load. Increments
        by 4 for every such cycle. Unit: cpu]
  exe_activity.bound_on_stores
       [Cycles where the Store Buffer was full and no loads caused an
        execution stall. Unit: cpu]
  exe_activity.exe_bound_0_ports
       [Cycles no uop executed while RS was not empty,the SB was not full and
        there was no outstanding load. Unit: cpu]
  ild_stall.lcp
       [Stalls caused by changing prefix length of the instruction. [This
        event is alias to DECODE.LCP]. Unit: cpu]
  inst_decoded.decoders
       [Instruction decoders utilized in a cycle. Unit: cpu]
  inst_retired.any
       [Number of instructions retired. Fixed Counter - architectural event
        (Precise event). Unit: cpu]
  inst_retired.any_p
       [Number of instructions retired. General Counter - architectural event
        (Precise event). Unit: cpu]
  inst_retired.nop
       [Retired NOP instructions (Precise event). Unit: cpu]
  inst_retired.prec_dist
       [Precise instruction retired event with a reduced effect of PEBS shadow
        in IP distribution (Precise event). Unit: cpu]
  int_misc.all_recovery_cycles
       [Cycles the Backend cluster is recovering after a miss-speculation or a
        Store Buffer or Load Buffer drain stall. Unit: cpu]
  int_misc.clear_resteer_cycles
       [Counts cycles after recovery from a branch misprediction or machine
        clear till the first uop is issued from the resteered path. Unit: cpu]
  int_misc.clears_count
       [Clears speculative count. Unit: cpu]
  int_misc.recovery_cycles
       [Core cycles the allocator was stalled due to recovery from earlier
        clear event for this thread. Unit: cpu]
  int_misc.uop_dropping
       [TMA slots where uops got dropped. Unit: cpu]
  ld_blocks.no_sr
       [The number of times that split load operations are temporarily blocked
        because all resources for handling the split accesses are in use.
        Unit: cpu]
  ld_blocks.store_forward
       [Loads blocked due to overlapping with a preceding store that cannot be
        forwarded. Unit: cpu]
  ld_blocks_partial.address_alias
       [False dependencies in MOB due to partial compare on address. Unit: cpu]
  load_hit_prefetch.swpf
       [Counts the number of demand load dispatches that hit L1D fill buffer
        (FB) allocated for software prefetch. Unit: cpu]
  lsd.cycles_active
       [Cycles Uops delivered by the LSD,but didn't come from the decoder.
        Unit: cpu]
  lsd.cycles_ok
       [Cycles optimal number of Uops delivered by the LSD,but did not come
        from the decoder. Unit: cpu]
  lsd.uops
       [Number of Uops delivered by the LSD. Unit: cpu]
  machine_clears.count
       [Number of machine clears (nukes) of any type. Unit: cpu]
  machine_clears.smc
       [Self-modifying code (SMC) detected. Unit: cpu]
  misc_retired.lbr_inserts
       [Increments whenever there is an update to the LBR array. Unit: cpu]
  misc_retired.pause_inst
       [Number of retired PAUSE instructions. This event is not supported on
        first SKL and KBL products. Unit: cpu]
  resource_stalls.sb
       [Cycles stalled due to no store buffers available. (not including
        draining form sync). Unit: cpu]
  resource_stalls.scoreboard
       [Counts cycles where the pipeline is stalled due to serializing
        operations. Unit: cpu]
  rs_events.empty_cycles
       [Cycles when Reservation Station (RS) is empty for the thread. Unit:
        cpu]
  rs_events.empty_end
       [Counts end of periods where the Reservation Station (RS) was empty.
        Unit: cpu]
  topdown.backend_bound_slots
       [TMA slots where no uops were being issued due to lack of back-end
        resources. Unit: cpu]
  topdown.slots
       [TMA slots available for an unhalted logical processor. Fixed counter -
        architectural event. Unit: cpu]
  topdown.slots_p
       [TMA slots available for an unhalted logical processor. General counter
        - architectural event. Unit: cpu]
  uops_decoded.dec0
       [Number of uops decoded out of instructions exclusively fetched by
        decoder 0. Unit: cpu]
  uops_dispatched.port_0
       [Number of uops executed on port 0. Unit: cpu]
  uops_dispatched.port_1
       [Number of uops executed on port 1. Unit: cpu]
  uops_dispatched.port_2_3
       [Number of uops executed on port 2 and 3. Unit: cpu]
  uops_dispatched.port_4_9
       [Number of uops executed on port 4 and 9. Unit: cpu]
  uops_dispatched.port_5
       [Number of uops executed on port 5. Unit: cpu]
  uops_dispatched.port_6
       [Number of uops executed on port 6. Unit: cpu]
  uops_dispatched.port_7_8
       [Number of uops executed on port 7 and 8. Unit: cpu]
  uops_executed.core
       [Number of uops executed on the core. Unit: cpu]
  uops_executed.core_cycles_ge_1
       [Cycles at least 1 micro-op is executed from any thread on physical
        core. Unit: cpu]
  uops_executed.core_cycles_ge_2
       [Cycles at least 2 micro-op is executed from any thread on physical
        core. Unit: cpu]
  uops_executed.core_cycles_ge_3
       [Cycles at least 3 micro-op is executed from any thread on physical
        core. Unit: cpu]
  uops_executed.core_cycles_ge_4
       [Cycles at least 4 micro-op is executed from any thread on physical
        core. Unit: cpu]
  uops_executed.cycles_ge_1
       [Cycles where at least 1 uop was executed per-thread. Unit: cpu]
  uops_executed.cycles_ge_2
       [Cycles where at least 2 uops were executed per-thread. Unit: cpu]
  uops_executed.cycles_ge_3
       [Cycles where at least 3 uops were executed per-thread. Unit: cpu]
  uops_executed.cycles_ge_4
       [Cycles where at least 4 uops were executed per-thread. Unit: cpu]
  uops_executed.stall_cycles
       [Counts number of cycles no uops were dispatched to be executed on this
        thread. Unit: cpu]
  uops_executed.thread
       [Counts the number of uops to be executed per-thread each cycle. Unit:
        cpu]
  uops_executed.x87
       [Counts the number of x87 uops dispatched. Unit: cpu]
  uops_issued.any
       [Uops that RAT issues to RS. Unit: cpu]
  uops_issued.stall_cycles
       [Cycles when RAT does not issue Uops to RS for the thread. Unit: cpu]
  uops_issued.vector_width_mismatch
       [Uops inserted at issue-stage in order to preserve upper bits of vector
        registers. Unit: cpu]
  uops_retired.slots
       [Retirement slots used. Unit: cpu]
  uops_retired.stall_cycles
       [Cycles without actually retired uops. Unit: cpu]
  uops_retired.total_cycles
       [Cycles with less than 10 actually retired uops. Unit: cpu]

uncore interconnect:
  unc_arb_coh_trk_requests.all
       [UNC_ARB_COH_TRK_REQUESTS.ALL. Unit: uncore_arb]
  unc_arb_dat_occupancy.all
       [Each cycle counts number of any coherent request at memory controller
        that were issued by any core. Unit: uncore_arb]
  unc_arb_dat_occupancy.rd
       [Each cycle counts number of coherent reads pending on data return from
        memory controller that were issued by any core. Unit: uncore_arb]
  unc_arb_req_trk_occupancy.drd
       [Each cycle count number of 'valid' coherent Data Read entries . Such
        entry is defined as valid when it is allocated till deallocation.
        Doesn't include prefetches [This event is alias to
        UNC_ARB_TRK_OCCUPANCY.RD]. Unit: uncore_arb]
  unc_arb_req_trk_request.drd
       [Number of all coherent Data Read entries. Doesn't include prefetches
        [This event is alias to UNC_ARB_TRK_REQUESTS.RD]. Unit: uncore_arb]
  unc_arb_trk_occupancy.all
       [Each cycle count number of all outgoing valid entries in ReqTrk. Such
        entry is defined as valid from it's allocation in ReqTrk till
        deallocation. Accounts for Coherent and non-coherent traffic. Unit:
        uncore_arb]
  unc_arb_trk_occupancy.rd
       [Each cycle count number of 'valid' coherent Data Read entries . Such
        entry is defined as valid when it is allocated till deallocation.
        Doesn't include prefetches [This event is alias to
        UNC_ARB_REQ_TRK_OCCUPANCY.DRD]. Unit: uncore_arb]
  unc_arb_trk_requests.all
       [UNC_ARB_TRK_REQUESTS.ALL. Unit: uncore_arb]
  unc_arb_trk_requests.rd
       [Number of all coherent Data Read entries. Doesn't include prefetches
        [This event is alias to UNC_ARB_REQ_TRK_REQUEST.DRD]. Unit: uncore_arb]

uncore memory:
  unc_mc0_rdcas_count_freerun
       [Counts every read (RdCAS) issued by the Memory Controller to DRAM (sum
        of all channels). All requests result in 64 byte data transfers from
        DRAM. Unit: uncore_imc_free_running_0]
  unc_mc0_total_reqcount_freerun
       [Counts every 64B read and write request entering the Memory Controller
        to DRAM (sum of all channels). Each write request counts as a new
        request incrementing this counter. However,same cache line write
        requests (both full and partial) are combined to a single 64 byte data
        transfer to DRAM. Unit: uncore_imc_free_running_0]
  unc_mc0_wrcas_count_freerun
       [Counts every write (WrCAS) issued by the Memory Controller to DRAM
        (sum of all channels). All requests result in 64 byte data transfers
        from DRAM. Unit: uncore_imc_free_running_0]

uncore other:
  unc_clock.socket
       [UNC_CLOCK.SOCKET. Unit: uncore_clock]

virtual memory:
  dtlb_load_misses.stlb_hit
       [Loads that miss the DTLB and hit the STLB. Unit: cpu]
  dtlb_load_misses.walk_active
       [Cycles when at least one PMH is busy with a page walk for a demand
        load. Unit: cpu]
  dtlb_load_misses.walk_completed
       [Load miss in all TLB levels causes a page walk that completes. (All
        page sizes). Unit: cpu]
  dtlb_load_misses.walk_completed_2m_4m
       [Page walks completed due to a demand data load to a 2M/4M page. Unit:
        cpu]
  dtlb_load_misses.walk_completed_4k
       [Page walks completed due to a demand data load to a 4K page. Unit: cpu]
  dtlb_load_misses.walk_pending
       [Number of page walks outstanding for a demand load in the PMH each
        cycle. Unit: cpu]
  dtlb_store_misses.stlb_hit
       [Stores that miss the DTLB and hit the STLB. Unit: cpu]
  dtlb_store_misses.walk_active
       [Cycles when at least one PMH is busy with a page walk for a store.
        Unit: cpu]
  dtlb_store_misses.walk_completed
       [Store misses in all TLB levels causes a page walk that completes. (All
        page sizes). Unit: cpu]
  dtlb_store_misses.walk_completed_2m_4m
       [Page walks completed due to a demand data store to a 2M/4M page. Unit:
        cpu]
  dtlb_store_misses.walk_completed_4k
       [Page walks completed due to a demand data store to a 4K page. Unit:
        cpu]
  dtlb_store_misses.walk_pending
       [Number of page walks outstanding for a store in the PMH each cycle.
        Unit: cpu]
  itlb_misses.stlb_hit
       [Instruction fetch requests that miss the ITLB and hit the STLB. Unit:
        cpu]
  itlb_misses.walk_active
       [Cycles when at least one PMH is busy with a page walk for code
        (instruction fetch) request. Unit: cpu]
  itlb_misses.walk_completed
       [Code miss in all TLB levels causes a page walk that completes. (All
        page sizes). Unit: cpu]
  itlb_misses.walk_completed_2m_4m
       [Code miss in all TLB levels causes a page walk that completes.
        (2M/4M). Unit: cpu]
  itlb_misses.walk_completed_4k
       [Code miss in all TLB levels causes a page walk that completes. (4K).
        Unit: cpu]
  itlb_misses.walk_pending
       [Number of page walks outstanding for an outstanding code request in
        the PMH each cycle. Unit: cpu]
  tlb_flush.dtlb_thread
       [DTLB flush attempts of the thread-specific entries. Unit: cpu]
  tlb_flush.stlb_any
       [STLB flush attempts. Unit: cpu]
  rNNN                                               [Raw event descriptor]
  cpu/event=0..255,pc,edge,.../modifier              [Raw event descriptor]
       [(see 'man perf-list' or 'man perf-record' on how to encode it)]
  breakpoint//modifier                               [Raw event descriptor]
  cstate_core/event=0..0xffffffffffffffff/modifier   [Raw event descriptor]
  cstate_pkg/event=0..0xffffffffffffffff/modifier    [Raw event descriptor]
  i915/i915_eventid=0..0x1fffff/modifier             [Raw event descriptor]
  intel_bts//modifier                                [Raw event descriptor]
  intel_pt/ptw,event,cyc_thresh=0..15,.../modifier   [Raw event descriptor]
  kprobe/retprobe/modifier                           [Raw event descriptor]
  msr/event=0..0xffffffffffffffff/modifier           [Raw event descriptor]
  power/event=0..255/modifier                        [Raw event descriptor]
  software//modifier                                 [Raw event descriptor]
  tracepoint//modifier                               [Raw event descriptor]
  uncore_arb/event=0..255,edge,inv,.../modifier      [Raw event descriptor]
  uncore_cbox/event=0..255,edge,inv,.../modifier     [Raw event descriptor]
  uncore_clock/event=0..255/modifier                 [Raw event descriptor]
  uncore_imc_free_running/event=0..255,umask=0..255/modifier[Raw event descriptor]
  uprobe/ref_ctr_offset=0..0xffffffff,retprobe/modifier[Raw event descriptor]
  mem:<addr>[/len][:access]                          [Hardware breakpoint]
  sdt_rtld:init_complete                             [SDT event]
  sdt_rtld:init_start                                [SDT event]
  sdt_rtld:lll_lock_wait                             [SDT event]
  sdt_rtld:lll_lock_wait_private                     [SDT event]
  sdt_rtld:longjmp                                   [SDT event]
  sdt_rtld:longjmp_target                            [SDT event]
  sdt_rtld:map_complete                              [SDT event]
  sdt_rtld:map_start                                 [SDT event]
  sdt_rtld:reloc_complete                            [SDT event]
  sdt_rtld:reloc_start                               [SDT event]
  sdt_rtld:setjmp                                    [SDT event]
  sdt_rtld:unmap_complete                            [SDT event]
  sdt_rtld:unmap_start                               [SDT event]

Metric Groups:

Backend: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_core_bound
       [This metric represents fraction of slots where Core non-memory issues
        were of a bottleneck]
  tma_info_core_ilp
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per thread (logical-processor)]
  tma_info_memory_l2mpki
       [L2 cache true misses per kilo instruction for retired demand loads]
  tma_memory_bound
       [This metric represents fraction of slots the Memory subsystem within
        the Backend was a bottleneck]

Bad:
  tma_info_bad_spec_branch_misprediction_cost
       [Branch Misprediction Cost: Fraction of TMA slots wasted per
        non-speculative branch misprediction (retired JEClear)]
  tma_info_bad_spec_ipmisp_cond_ntaken
       [Instructions per retired mispredicts for conditional non-taken
        branches (lower number means higher occurrence rate)]
  tma_info_bad_spec_ipmisp_cond_taken
       [Instructions per retired mispredicts for conditional taken branches
        (lower number means higher occurrence rate)]
  tma_info_bad_spec_ipmisp_indirect
       [Instructions per retired mispredicts for indirect CALL or JMP branches
        (lower number means higher occurrence rate)]
  tma_info_bad_spec_ipmisp_ret
       [Instructions per retired mispredicts for return branches (lower number
        means higher occurrence rate)]
  tma_info_bad_spec_ipmispredict
       [Number of Instructions per non-speculative Branch Misprediction
        (JEClear) (lower number means higher occurrence rate)]
  tma_info_bottleneck_irregular_overhead
       [Total pipeline cost of irregular execution (e.g]
  tma_info_bottleneck_mispredictions
       [Total pipeline cost of Branch Misprediction related bottlenecks]
  tma_info_branches_callret
       [Fraction of branches that are CALL or RET]
  tma_info_branches_cond_nt
       [Fraction of branches that are non-taken conditionals]
  tma_info_branches_cond_tk
       [Fraction of branches that are taken conditionals]
  tma_info_branches_jump
       [Fraction of branches that are unconditional (direct or indirect) jumps]
  tma_info_branches_other_branches
       [Fraction of branches of other types (not individually covered by other
        metrics in Info.Branches group)]

BadSpec: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_clears_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Machine Clears]
  tma_info_bad_spec_ipmispredict
       [Number of Instructions per non-speculative Branch Misprediction
        (JEClear) (lower number means higher occurrence rate)]
  tma_info_bottleneck_mispredictions
       [Total pipeline cost of Branch Misprediction related bottlenecks]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]
  tma_mispredicts_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Branch Misprediction at execution stage]

BigFootprint: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_icache_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        instruction cache misses]
  tma_info_bottleneck_big_code
       [Total pipeline cost of instruction fetch related bottlenecks by large
        code footprint programs (i-side cache; TLB and BTB misses)]
  tma_itlb_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        Instruction TLB (ITLB) misses]
  tma_unknown_branches
       [This metric represents fraction of cycles the CPU was stalled due to
        new branch address clears]

BrMispredicts: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_info_bad_spec_branch_misprediction_cost
       [Branch Misprediction Cost: Fraction of TMA slots wasted per
        non-speculative branch misprediction (retired JEClear)]
  tma_info_bad_spec_ipmisp_cond_ntaken
       [Instructions per retired mispredicts for conditional non-taken
        branches (lower number means higher occurrence rate)]
  tma_info_bad_spec_ipmisp_cond_taken
       [Instructions per retired mispredicts for conditional taken branches
        (lower number means higher occurrence rate)]
  tma_info_bad_spec_ipmisp_indirect
       [Instructions per retired mispredicts for indirect CALL or JMP branches
        (lower number means higher occurrence rate)]
  tma_info_bad_spec_ipmisp_ret
       [Instructions per retired mispredicts for return branches (lower number
        means higher occurrence rate)]
  tma_info_bad_spec_ipmispredict
       [Number of Instructions per non-speculative Branch Misprediction
        (JEClear) (lower number means higher occurrence rate)]
  tma_info_bad_spec_spec_clears_ratio
       [Speculative to Retired ratio of all clears (covering mispredicts and
        nukes)]
  tma_info_bottleneck_mispredictions
       [Total pipeline cost of Branch Misprediction related bottlenecks]
  tma_mispredicts_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Branch Misprediction at execution stage]
  tma_other_mispredicts
       [This metric estimates fraction of slots the CPU was stalled due to
        other cases of misprediction (non-retired x86 branches or other types)]

Branches: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_branch_instructions
       [This metric represents fraction of slots where the CPU was retiring
        branch instructions]
  tma_info_branches_callret
       [Fraction of branches that are CALL or RET]
  tma_info_branches_cond_nt
       [Fraction of branches that are non-taken conditionals]
  tma_info_branches_cond_tk
       [Fraction of branches that are taken conditionals]
  tma_info_branches_jump
       [Fraction of branches that are unconditional (direct or indirect) jumps]
  tma_info_branches_other_branches
       [Fraction of branches of other types (not individually covered by other
        metrics in Info.Branches group)]
  tma_info_inst_mix_bptkbranch
       [Branch instructions per taken branch]
  tma_info_inst_mix_ipbranch
       [Instructions per Branch (lower number means higher occurrence rate)]
  tma_info_inst_mix_ipcall
       [Instructions per (near) call (lower number means higher occurrence
        rate)]
  tma_info_inst_mix_iptb
       [Instructions per taken branch]
  tma_info_system_ipfarbranch
       [Instructions per Far Branch ( Far Branches apply upon transition from
        application to operating system,handling interrupts,exceptions) [lower
        number means higher occurrence rate]]
  tma_info_thread_uptb
       [Uops per taken branch]

BvBC: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_icache_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        instruction cache misses]
  tma_info_bottleneck_big_code
       [Total pipeline cost of instruction fetch related bottlenecks by large
        code footprint programs (i-side cache; TLB and BTB misses)]
  tma_itlb_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        Instruction TLB (ITLB) misses]
  tma_unknown_branches
       [This metric represents fraction of cycles the CPU was stalled due to
        new branch address clears]

BvBO: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_branch_instructions
       [This metric represents fraction of slots where the CPU was retiring
        branch instructions]
  tma_info_bottleneck_branching_overhead
       [Total pipeline cost of instructions used for program control-flow - a
        subset of the Retiring category in TMA]
  tma_nop_instructions
       [This metric represents fraction of slots where the CPU was retiring
        NOP (no op) instructions]

BvCB: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_divider
       [This metric represents fraction of cycles where the Divider unit was
        active]
  tma_info_bottleneck_compute_bound_est
       [Total pipeline cost when the execution is compute-bound - an
        estimation]
  tma_ports_utilized_3m
       [This metric represents fraction of cycles CPU executed total of 3 or
        more uops per cycle on all execution ports (Logical Processor cycles
        since ICL,Physical Core cycles otherwise)]

BvFB: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_frontend_bound
       [This category represents fraction of slots where the processor's
        Frontend undersupplies its Backend]
  tma_info_bottleneck_instruction_fetch_bw
       [Total pipeline cost of instruction fetch bandwidth related bottlenecks
        (when the front-end could not sustain operations delivery to the
        back-end)]

BvIO: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_assists
       [This metric estimates fraction of slots the CPU retired uops delivered
        by the Microcode_Sequencer as a result of Assists]
  tma_frontend_bound
       [This category represents fraction of slots where the processor's
        Frontend undersupplies its Backend]
  tma_info_bottleneck_irregular_overhead
       [Total pipeline cost of irregular execution (e.g]
  tma_other_mispredicts
       [This metric estimates fraction of slots the CPU was stalled due to
        other cases of misprediction (non-retired x86 branches or other types)]
  tma_other_nukes
       [This metric represents fraction of slots the CPU has wasted due to
        Nukes (Machine Clears) not related to memory ordering]
  tma_serializing_operation
       [This metric represents fraction of cycles the CPU issue-pipeline was
        stalled due to serializing operations]

BvMB: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_bottleneck_cache_memory_bandwidth
       [Total pipeline cost of external Memory- or Cache-Bandwidth related
        bottlenecks]

BvML: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_bottleneck_cache_memory_latency
       [Total pipeline cost of external Memory- or Cache-Latency related
        bottlenecks]
  tma_l1_hit_latency
       [This metric roughly estimates fraction of cycles with demand load
        accesses that hit the L1 cache]
  tma_l2_bound
       [This metric estimates how often the CPU was stalled due to L2 cache
        accesses by loads]
  tma_l3_hit_latency
       [This metric estimates fraction of cycles with demand load accesses
        that hit the L3 cache under unloaded scenarios (possibly L3 latency
        limited)]
  tma_mem_latency
       [This metric estimates fraction of cycles where the performance was
        likely hurt due to latency from external memory - DRAM ([SPR-HBM]
        and/or HBM)]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]

BvMP: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_info_bottleneck_mispredictions
       [Total pipeline cost of Branch Misprediction related bottlenecks]
  tma_mispredicts_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Branch Misprediction at execution stage]

BvMS: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_data_sharing
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to data-sharing accesses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]
  tma_info_bottleneck_memory_synchronization
       [Total pipeline cost of Memory Synchronization related bottlenecks
        (data transfers and coherency updates across processors)]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]
  tma_mem_bandwidth
       [This metric estimates fraction of cycles where the core's performance
        was likely hurt due to approaching bandwidth limits of external memory
        - DRAM ([SPR-HBM] and/or HBM)]
  tma_sq_full
       [This metric measures fraction of cycles where the Super Queue (SQ) was
        full taking into account all request-types and both hardware SMT
        threads (Logical Processors)]

BvMT: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_dtlb_load
       [This metric roughly estimates the fraction of cycles where the Data
        TLB (DTLB) was missed by load accesses]
  tma_dtlb_store
       [This metric roughly estimates the fraction of cycles spent handling
        first-level data TLB store misses]
  tma_info_bottleneck_memory_data_tlbs
       [Total pipeline cost of Memory Address Translation related bottlenecks
        (data-side TLBs)]

BvOB: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_backend_bound
       [This category represents fraction of slots where no uops are being
        delivered due to a lack of required resources for accepting new uops
        in the Backend]
  tma_info_bottleneck_other_bottlenecks
       [Total pipeline cost of remaining bottlenecks in the back-end]

BvUW: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_bottleneck_useful_work
       [Total pipeline cost of "useful operations" - the portion of Retiring
        category not covered by Branching_Overhead nor Irregular_Overhead]
  tma_retiring
       [This category represents fraction of slots utilized by useful work
        i.e. issued uops that eventually get retired]

CacheHits: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_memory_fb_hpki
       [Fill Buffer (FB) hits per kilo instructions for retired demand loads
        (L1D misses that merge into ongoing miss-handling entries)]
  tma_info_memory_l1mpki
       [L1 cache true misses per kilo instruction for retired demand loads]
  tma_info_memory_l1mpki_load
       [L1 cache true misses per kilo instruction for all demand loads
        (including speculative)]
  tma_info_memory_l2hpki_all
       [L2 cache hits per kilo instruction for all request types (including
        speculative)]
  tma_info_memory_l2hpki_load
       [L2 cache hits per kilo instruction for all demand loads (including
        speculative)]
  tma_info_memory_l2mpki
       [L2 cache true misses per kilo instruction for retired demand loads]
  tma_info_memory_l2mpki_all
       [L2 cache ([RKL+] true) misses per kilo instruction for all request
        types (including speculative)]
  tma_info_memory_l2mpki_load
       [L2 cache ([RKL+] true) misses per kilo instruction for all demand
        loads (including speculative)]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_l2_bound
       [This metric estimates how often the CPU was stalled due to L2 cache
        accesses by loads]
  tma_l3_bound
       [This metric estimates how often the CPU was stalled due to loads
        accesses to L3 cache or contended with a sibling Core]

CacheMisses: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_memory_l2mpki_rfo
       [Offcore requests (L2 cache miss) per kilo instruction for demand RFOs]

CodeGen: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_branches_cond_nt
       [Fraction of branches that are non-taken conditionals]
  tma_info_branches_cond_tk
       [Fraction of branches that are taken conditionals]

Compute: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_core_bound
       [This metric represents fraction of slots where Core non-memory issues
        were of a bottleneck]
  tma_fp_scalar
       [This metric approximates arithmetic floating-point (FP) scalar uops
        fraction the CPU has retired]
  tma_fp_vector
       [This metric approximates arithmetic floating-point (FP) vector uops
        fraction the CPU has retired aggregated across all vector widths]
  tma_fp_vector_128b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 128-bit wide vectors]
  tma_fp_vector_256b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 256-bit wide vectors]
  tma_fp_vector_512b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 512-bit wide vectors]
  tma_port_0
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 0 ([SNB+] ALU; [HSW+] ALU and 2nd branch)]
  tma_x87_use
       [This metric serves as an approximation of legacy x87 usage]

Cor: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_botlnk_l0_core_bound_likely
       [Probability of Core Bound bottleneck hidden by SMT-profiling artifacts]
  tma_info_bottleneck_compute_bound_est
       [Total pipeline cost when the execution is compute-bound - an
        estimation]
  tma_info_bottleneck_irregular_overhead
       [Total pipeline cost of irregular execution (e.g]
  tma_info_bottleneck_other_bottlenecks
       [Total pipeline cost of remaining bottlenecks in the back-end]
  tma_info_core_fp_arith_utilization
       [Actual per-core usage of the Floating Point non-X87 execution units
        (regardless of precision or vector-width)]
  tma_info_core_ilp
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per thread (logical-processor)]
  tma_info_pipeline_execute
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per core]
  tma_info_system_gflops
       [Giga Floating Point Operations Per Second]
  tma_info_thread_execute_per_issue
       [The ratio of Executed- by Issued-Uops]

DSB: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_dsb
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to DSB (decoded uop cache) fetch pipeline]
  tma_info_botlnk_l2_dsb_bandwidth
       [Total pipeline cost of DSB (uop cache) hits - subset of the
        Instruction_Fetch_BW Bottleneck]
  tma_info_frontend_dsb_coverage
       [Fraction of Uops delivered by the DSB (aka Decoded ICache; or Uop
        Cache)]

DSBmiss: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_decoder0_alone
       [This metric represents fraction of cycles where decoder-0 was the only
        active decoder]
  tma_dsb_switches
       [This metric represents fraction of cycles the CPU was stalled due to
        switches from DSB to MITE pipelines]
  tma_info_botlnk_l2_dsb_misses
       [Total pipeline cost of DSB (uop cache) misses - subset of the
        Instruction_Fetch_BW Bottleneck]
  tma_info_frontend_dsb_switch_cost
       [Average number of cycles of a switch from the DSB fetch-unit to MITE
        fetch unit - see DSB_Switches tree node for details]
  tma_info_frontend_ipdsb_miss_ret
       [Instructions per non-speculative DSB miss (lower number means higher
        occurrence rate)]
  tma_mite
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to the MITE pipeline (the legacy decode pipeline)]
  tma_mite_4wide
       [This metric represents fraction of cycles where (only) 4 uops were
        delivered by the MITE pipeline]

DataSharing: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]

Default:
  tma_backend_bound
       [This category represents fraction of slots where no uops are being
        delivered due to a lack of required resources for accepting new uops
        in the Backend]
  tma_bad_speculation
       [This category represents fraction of slots wasted due to incorrect
        speculations]
  tma_frontend_bound
       [This category represents fraction of slots where the processor's
        Frontend undersupplies its Backend]
  tma_retiring
       [This category represents fraction of slots utilized by useful work
        i.e. issued uops that eventually get retired]

Fed: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_botlnk_l2_dsb_misses
       [Total pipeline cost of DSB (uop cache) misses - subset of the
        Instruction_Fetch_BW Bottleneck]
  tma_info_botlnk_l2_ic_misses
       [Total pipeline cost of Instruction Cache misses - subset of the
        Big_Code Bottleneck]
  tma_info_bottleneck_big_code
       [Total pipeline cost of instruction fetch related bottlenecks by large
        code footprint programs (i-side cache; TLB and BTB misses)]
  tma_info_bottleneck_instruction_fetch_bw
       [Total pipeline cost of instruction fetch bandwidth related bottlenecks
        (when the front-end could not sustain operations delivery to the
        back-end)]
  tma_info_frontend_dsb_coverage
       [Fraction of Uops delivered by the DSB (aka Decoded ICache; or Uop
        Cache)]
  tma_info_frontend_fetch_upc
       [Average number of Uops issued by front-end when it issued something]
  tma_info_frontend_icache_miss_latency
       [Average Latency for L1 instruction cache misses]
  tma_info_frontend_ipdsb_miss_ret
       [Instructions per non-speculative DSB miss (lower number means higher
        occurrence rate)]
  tma_info_frontend_ipunknown_branch
       [Instructions per speculative Unknown Branch Misprediction (BAClear)
        (lower number means higher occurrence rate)]
  tma_info_frontend_lsd_coverage
       [Fraction of Uops delivered by the LSD (Loop Stream Detector; aka Loop
        Cache)]
  tma_info_inst_mix_bptkbranch
       [Branch instructions per taken branch]
  tma_info_inst_mix_ipbranch
       [Instructions per Branch (lower number means higher occurrence rate)]
  tma_info_inst_mix_ipcall
       [Instructions per (near) call (lower number means higher occurrence
        rate)]
  tma_info_inst_mix_iptb
       [Instructions per taken branch]
  tma_info_memory_tlb_code_stlb_mpki
       [STLB (2nd level TLB) code speculative misses per kilo instruction
        (misses of any page-size that complete the page walk)]
  tma_info_pipeline_fetch_dsb
       [Average number of uops fetched from DSB per cycle]
  tma_info_pipeline_fetch_lsd
       [Average number of uops fetched from LSD per cycle]
  tma_info_pipeline_fetch_mite
       [Average number of uops fetched from MITE per cycle]
  tma_info_thread_uptb
       [Uops per taken branch]

FetchBW: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_decoder0_alone
       [This metric represents fraction of cycles where decoder-0 was the only
        active decoder]
  tma_dsb
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to DSB (decoded uop cache) fetch pipeline]
  tma_fetch_bandwidth
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend bandwidth issues]
  tma_info_botlnk_l2_dsb_bandwidth
       [Total pipeline cost of DSB (uop cache) hits - subset of the
        Instruction_Fetch_BW Bottleneck]
  tma_info_bottleneck_instruction_fetch_bw
       [Total pipeline cost of instruction fetch bandwidth related bottlenecks
        (when the front-end could not sustain operations delivery to the
        back-end)]
  tma_info_frontend_dsb_coverage
       [Fraction of Uops delivered by the DSB (aka Decoded ICache; or Uop
        Cache)]
  tma_info_frontend_fetch_upc
       [Average number of Uops issued by front-end when it issued something]
  tma_info_inst_mix_iptb
       [Instructions per taken branch]
  tma_info_pipeline_fetch_dsb
       [Average number of uops fetched from DSB per cycle]
  tma_info_pipeline_fetch_lsd
       [Average number of uops fetched from LSD per cycle]
  tma_info_pipeline_fetch_mite
       [Average number of uops fetched from MITE per cycle]
  tma_info_thread_uptb
       [Uops per taken branch]
  tma_lsd
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to LSD (Loop Stream Detector) unit]
  tma_mite
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to the MITE pipeline (the legacy decode pipeline)]
  tma_mite_4wide
       [This metric represents fraction of cycles where (only) 4 uops were
        delivered by the MITE pipeline]

FetchLat: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_branch_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers]
  tma_dsb_switches
       [This metric represents fraction of cycles the CPU was stalled due to
        switches from DSB to MITE pipelines]
  tma_icache_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        instruction cache misses]
  tma_info_botlnk_l2_ic_misses
       [Total pipeline cost of Instruction Cache misses - subset of the
        Big_Code Bottleneck]
  tma_info_frontend_icache_miss_latency
       [Average Latency for L1 instruction cache misses]
  tma_itlb_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        Instruction TLB (ITLB) misses]
  tma_lcp
       [This metric represents fraction of cycles CPU was stalled due to
        Length Changing Prefixes (LCPs)]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]
  tma_unknown_branches
       [This metric represents fraction of cycles the CPU was stalled due to
        new branch address clears]

Flops: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_fp_scalar
       [This metric approximates arithmetic floating-point (FP) scalar uops
        fraction the CPU has retired]
  tma_fp_vector
       [This metric approximates arithmetic floating-point (FP) vector uops
        fraction the CPU has retired aggregated across all vector widths]
  tma_fp_vector_128b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 128-bit wide vectors]
  tma_fp_vector_256b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 256-bit wide vectors]
  tma_fp_vector_512b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 512-bit wide vectors]
  tma_info_core_flopc
       [Floating Point Operations Per Cycle]
  tma_info_core_fp_arith_utilization
       [Actual per-core usage of the Floating Point non-X87 execution units
        (regardless of precision or vector-width)]
  tma_info_inst_mix_iparith
       [Instructions per FP Arithmetic instruction (lower number means higher
        occurrence rate)]
  tma_info_inst_mix_iparith_avx128
       [Instructions per FP Arithmetic AVX/SSE 128-bit instruction (lower
        number means higher occurrence rate)]
  tma_info_inst_mix_iparith_avx256
       [Instructions per FP Arithmetic AVX* 256-bit instruction (lower number
        means higher occurrence rate)]
  tma_info_inst_mix_iparith_avx512
       [Instructions per FP Arithmetic AVX 512-bit instruction (lower number
        means higher occurrence rate)]
  tma_info_inst_mix_iparith_scalar_dp
       [Instructions per FP Arithmetic Scalar Double-Precision instruction
        (lower number means higher occurrence rate)]
  tma_info_inst_mix_iparith_scalar_sp
       [Instructions per FP Arithmetic Scalar Single-Precision instruction
        (lower number means higher occurrence rate)]
  tma_info_inst_mix_ipflop
       [Instructions per Floating Point (FP) Operation (lower number means
        higher occurrence rate)]
  tma_info_inst_mix_ippause
       [Instructions per PAUSE (lower number means higher occurrence rate)]
  tma_info_system_gflops
       [Giga Floating Point Operations Per Second]

FpScalar: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_inst_mix_iparith_scalar_dp
       [Instructions per FP Arithmetic Scalar Double-Precision instruction
        (lower number means higher occurrence rate)]
  tma_info_inst_mix_iparith_scalar_sp
       [Instructions per FP Arithmetic Scalar Single-Precision instruction
        (lower number means higher occurrence rate)]

FpVector: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_inst_mix_iparith_avx128
       [Instructions per FP Arithmetic AVX/SSE 128-bit instruction (lower
        number means higher occurrence rate)]
  tma_info_inst_mix_iparith_avx256
       [Instructions per FP Arithmetic AVX* 256-bit instruction (lower number
        means higher occurrence rate)]
  tma_info_inst_mix_iparith_avx512
       [Instructions per FP Arithmetic AVX 512-bit instruction (lower number
        means higher occurrence rate)]
  tma_info_inst_mix_ippause
       [Instructions per PAUSE (lower number means higher occurrence rate)]

Frontend: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_fetch_bandwidth
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend bandwidth issues]
  tma_fetch_latency
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend latency issues]
  tma_info_bottleneck_big_code
       [Total pipeline cost of instruction fetch related bottlenecks by large
        code footprint programs (i-side cache; TLB and BTB misses)]
  tma_info_bottleneck_instruction_fetch_bw
       [Total pipeline cost of instruction fetch bandwidth related bottlenecks
        (when the front-end could not sustain operations delivery to the
        back-end)]
  tma_info_inst_mix_iptb
       [Instructions per taken branch]

HPC: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_fp_arith
       [This metric represents overall arithmetic floating-point (FP)
        operations fraction the CPU has executed (retired)]
  tma_fp_assists
       [This metric roughly estimates fraction of slots the CPU retired uops
        as a result of handing Floating Point (FP) Assists]
  tma_info_core_fp_arith_utilization
       [Actual per-core usage of the Floating Point non-X87 execution units
        (regardless of precision or vector-width)]
  tma_info_system_cpu_utilization
       [Average CPU Utilization (percentage)]
  tma_info_system_dram_bw_use
       [Average external Memory Bandwidth Use for reads and writes [GB / sec]]
  tma_info_system_gflops
       [Giga Floating Point Operations Per Second]

IcMiss: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_icache_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        instruction cache misses]
  tma_info_botlnk_l2_ic_misses
       [Total pipeline cost of Instruction Cache misses - subset of the
        Big_Code Bottleneck]
  tma_info_bottleneck_big_code
       [Total pipeline cost of instruction fetch related bottlenecks by large
        code footprint programs (i-side cache; TLB and BTB misses)]
  tma_info_frontend_icache_miss_latency
       [Average Latency for L1 instruction cache misses]
  tma_info_frontend_l2mpki_code
       [L2 cache true code cacheline misses per kilo instruction]
  tma_info_frontend_l2mpki_code_all
       [L2 cache speculative code cacheline misses per kilo instruction]

InsType: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_inst_mix_iparith
       [Instructions per FP Arithmetic instruction (lower number means higher
        occurrence rate)]
  tma_info_inst_mix_iparith_avx128
       [Instructions per FP Arithmetic AVX/SSE 128-bit instruction (lower
        number means higher occurrence rate)]
  tma_info_inst_mix_iparith_avx256
       [Instructions per FP Arithmetic AVX* 256-bit instruction (lower number
        means higher occurrence rate)]
  tma_info_inst_mix_iparith_avx512
       [Instructions per FP Arithmetic AVX 512-bit instruction (lower number
        means higher occurrence rate)]
  tma_info_inst_mix_iparith_scalar_dp
       [Instructions per FP Arithmetic Scalar Double-Precision instruction
        (lower number means higher occurrence rate)]
  tma_info_inst_mix_iparith_scalar_sp
       [Instructions per FP Arithmetic Scalar Single-Precision instruction
        (lower number means higher occurrence rate)]
  tma_info_inst_mix_ipbranch
       [Instructions per Branch (lower number means higher occurrence rate)]
  tma_info_inst_mix_ipflop
       [Instructions per Floating Point (FP) Operation (lower number means
        higher occurrence rate)]
  tma_info_inst_mix_ipload
       [Instructions per Load (lower number means higher occurrence rate)]
  tma_info_inst_mix_ippause
       [Instructions per PAUSE (lower number means higher occurrence rate)]
  tma_info_inst_mix_ipstore
       [Instructions per Store (lower number means higher occurrence rate)]

LSD: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_frontend_lsd_coverage
       [Fraction of Uops delivered by the LSD (Loop Stream Detector; aka Loop
        Cache)]
  tma_lsd
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to LSD (Loop Stream Detector) unit]

MachineClears: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_clears_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Machine Clears]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]

Machine_Clears: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_other_nukes
       [This metric represents fraction of slots the CPU has wasted due to
        Nukes (Machine Clears) not related to memory ordering]

Mem:
  tma_info_bottleneck_cache_memory_bandwidth
       [Total pipeline cost of external Memory- or Cache-Bandwidth related
        bottlenecks]
  tma_info_bottleneck_cache_memory_latency
       [Total pipeline cost of external Memory- or Cache-Latency related
        bottlenecks]
  tma_info_bottleneck_memory_data_tlbs
       [Total pipeline cost of Memory Address Translation related bottlenecks
        (data-side TLBs)]
  tma_info_bottleneck_memory_synchronization
       [Total pipeline cost of Memory Synchronization related bottlenecks
        (data transfers and coherency updates across processors)]
  tma_info_memory_core_l1d_cache_fill_bw_2t
       [Average per-core data fill bandwidth to the L1 data cache [GB / sec]]
  tma_info_memory_core_l2_cache_fill_bw_2t
       [Average per-core data fill bandwidth to the L2 cache [GB / sec]]
  tma_info_memory_core_l3_cache_access_bw_2t
       [Average per-core data access bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_core_l3_cache_fill_bw_2t
       [Average per-core data fill bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_fb_hpki
       [Fill Buffer (FB) hits per kilo instructions for retired demand loads
        (L1D misses that merge into ongoing miss-handling entries)]
  tma_info_memory_l1d_cache_fill_bw
       [Average per-thread data fill bandwidth to the L1 data cache [GB / sec]]
  tma_info_memory_l1mpki
       [L1 cache true misses per kilo instruction for retired demand loads]
  tma_info_memory_l1mpki_load
       [L1 cache true misses per kilo instruction for all demand loads
        (including speculative)]
  tma_info_memory_l2_cache_fill_bw
       [Average per-thread data fill bandwidth to the L2 cache [GB / sec]]
  tma_info_memory_l2hpki_all
       [L2 cache hits per kilo instruction for all request types (including
        speculative)]
  tma_info_memory_l2hpki_load
       [L2 cache hits per kilo instruction for all demand loads (including
        speculative)]
  tma_info_memory_l2mpki
       [L2 cache true misses per kilo instruction for retired demand loads]
  tma_info_memory_l2mpki_all
       [L2 cache ([RKL+] true) misses per kilo instruction for all request
        types (including speculative)]
  tma_info_memory_l2mpki_load
       [L2 cache ([RKL+] true) misses per kilo instruction for all demand
        loads (including speculative)]
  tma_info_memory_l3_cache_access_bw
       [Average per-thread data access bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_l3_cache_fill_bw
       [Average per-thread data fill bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_l3mpki
       [L3 cache true misses per kilo instruction for retired demand loads]
  tma_info_memory_load_miss_real_latency
       [Actual Average Latency for L1 data-cache miss demand load operations
        (in core cycles)]
  tma_info_memory_mix_bus_lock_pki
       ["Bus lock" per kilo instruction]
  tma_info_memory_mix_uc_load_pki
       [Un-cacheable retired load per kilo instruction]
  tma_info_memory_mlp
       [Memory-Level-Parallelism (average number of L1 miss demand load when
        there is at least one such miss]
  tma_info_memory_tlb_load_stlb_mpki
       [STLB (2nd level TLB) data load speculative misses per kilo instruction
        (misses of any page-size that complete the page walk)]
  tma_info_memory_tlb_page_walks_utilization
       [Utilization of the core's Page Walker(s) serving STLB misses triggered
        by instruction/Load/Store accesses]
  tma_info_memory_tlb_store_stlb_mpki
       [STLB (2nd level TLB) data store speculative misses per kilo
        instruction (misses of any page-size that complete the page walk)]
  tma_info_system_mem_parallel_reads
       [Average number of parallel data read requests to external memory]
  tma_info_system_mem_read_latency
       [Average latency of data read request to external memory (in
        nanoseconds)]
  tma_info_thread_cpi
       [Cycles Per Instruction (per Logical Processor)]

MemOffcore: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_system_dram_bw_use
       [Average external Memory Bandwidth Use for reads and writes [GB / sec]]

MemoryBW: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]
  tma_info_bottleneck_cache_memory_bandwidth
       [Total pipeline cost of external Memory- or Cache-Bandwidth related
        bottlenecks]
  tma_info_memory_core_l1d_cache_fill_bw_2t
       [Average per-core data fill bandwidth to the L1 data cache [GB / sec]]
  tma_info_memory_core_l2_cache_fill_bw_2t
       [Average per-core data fill bandwidth to the L2 cache [GB / sec]]
  tma_info_memory_core_l3_cache_access_bw_2t
       [Average per-core data access bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_core_l3_cache_fill_bw_2t
       [Average per-core data fill bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_l1d_cache_fill_bw
       [Average per-thread data fill bandwidth to the L1 data cache [GB / sec]]
  tma_info_memory_l2_cache_fill_bw
       [Average per-thread data fill bandwidth to the L2 cache [GB / sec]]
  tma_info_memory_l3_cache_access_bw
       [Average per-thread data access bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_l3_cache_fill_bw
       [Average per-thread data fill bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_mlp
       [Memory-Level-Parallelism (average number of L1 miss demand load when
        there is at least one such miss]
  tma_info_system_dram_bw_use
       [Average external Memory Bandwidth Use for reads and writes [GB / sec]]
  tma_info_system_mem_parallel_reads
       [Average number of parallel data read requests to external memory]
  tma_mem_bandwidth
       [This metric estimates fraction of cycles where the core's performance
        was likely hurt due to approaching bandwidth limits of external memory
        - DRAM ([SPR-HBM] and/or HBM)]
  tma_sq_full
       [This metric measures fraction of cycles where the Super Queue (SQ) was
        full taking into account all request-types and both hardware SMT
        threads (Logical Processors)]
  tma_streaming_stores
       [This metric estimates how often CPU was stalled due to Streaming store
        memory accesses; Streaming store optimize out a read request required
        by RFO stores]

MemoryBound: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_dram_bound
       [This metric estimates how often the CPU was stalled on accesses to
        external memory (DRAM) by loads]
  tma_info_memory_load_miss_real_latency
       [Actual Average Latency for L1 data-cache miss demand load operations
        (in core cycles)]
  tma_info_memory_mlp
       [Memory-Level-Parallelism (average number of L1 miss demand load when
        there is at least one such miss]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_l2_bound
       [This metric estimates how often the CPU was stalled due to L2 cache
        accesses by loads]
  tma_l3_bound
       [This metric estimates how often the CPU was stalled due to loads
        accesses to L3 cache or contended with a sibling Core]
  tma_store_bound
       [This metric estimates how often CPU was stalled due to RFO store
        memory accesses; RFO store issue a read-for-ownership request before
        the write]

MemoryLat: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_bottleneck_cache_memory_latency
       [Total pipeline cost of external Memory- or Cache-Latency related
        bottlenecks]
  tma_info_memory_load_miss_real_latency
       [Actual Average Latency for L1 data-cache miss demand load operations
        (in core cycles)]
  tma_info_system_mem_read_latency
       [Average latency of data read request to external memory (in
        nanoseconds)]
  tma_l1_hit_latency
       [This metric roughly estimates fraction of cycles with demand load
        accesses that hit the L1 cache]
  tma_l3_hit_latency
       [This metric estimates fraction of cycles with demand load accesses
        that hit the L3 cache under unloaded scenarios (possibly L3 latency
        limited)]
  tma_mem_latency
       [This metric estimates fraction of cycles where the performance was
        likely hurt due to latency from external memory - DRAM ([SPR-HBM]
        and/or HBM)]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]

MemoryTLB: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_dtlb_load
       [This metric roughly estimates the fraction of cycles where the Data
        TLB (DTLB) was missed by load accesses]
  tma_dtlb_store
       [This metric roughly estimates the fraction of cycles spent handling
        first-level data TLB store misses]
  tma_info_bottleneck_big_code
       [Total pipeline cost of instruction fetch related bottlenecks by large
        code footprint programs (i-side cache; TLB and BTB misses)]
  tma_info_bottleneck_memory_data_tlbs
       [Total pipeline cost of Memory Address Translation related bottlenecks
        (data-side TLBs)]
  tma_info_memory_tlb_code_stlb_mpki
       [STLB (2nd level TLB) code speculative misses per kilo instruction
        (misses of any page-size that complete the page walk)]
  tma_info_memory_tlb_load_stlb_mpki
       [STLB (2nd level TLB) data load speculative misses per kilo instruction
        (misses of any page-size that complete the page walk)]
  tma_info_memory_tlb_page_walks_utilization
       [Utilization of the core's Page Walker(s) serving STLB misses triggered
        by instruction/Load/Store accesses]
  tma_info_memory_tlb_store_stlb_mpki
       [STLB (2nd level TLB) data store speculative misses per kilo
        instruction (misses of any page-size that complete the page walk)]
  tma_itlb_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        Instruction TLB (ITLB) misses]
  tma_load_stlb_hit
       [This metric roughly estimates the fraction of cycles where the (first
        level) DTLB was missed by load accesses,that later on hit in
        second-level TLB (STLB)]
  tma_load_stlb_miss
       [This metric estimates the fraction of cycles where the Second-level
        TLB (STLB) was missed by load accesses,performing a hardware page walk]
  tma_store_stlb_hit
       [This metric roughly estimates the fraction of cycles where the TLB was
        missed by store accesses,hitting in the second-level TLB (STLB)]
  tma_store_stlb_miss
       [This metric estimates the fraction of cycles where the STLB was missed
        by store accesses,performing a hardware page walk]

Memory_BW: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_memory_latency_data_l2_mlp
       [Average Parallel L2 cache miss data reads]
  tma_info_memory_latency_load_l2_mlp
       [Average Parallel L2 cache miss demand Loads]

Memory_Lat: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_memory_latency_load_l2_miss_latency
       [Average Latency for L2 cache miss demand Loads]
  tma_info_memory_latency_load_l3_miss_latency
       [Average Latency for L3 cache miss demand Loads]

MicroSeq: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_pipeline_ipassist
       [Instructions per a microcode Assist invocation]
  tma_microcode_sequencer
       [This metric represents fraction of slots the CPU was retiring uops
        fetched by the Microcode Sequencer (MS) unit]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]

OS: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_system_ipfarbranch
       [Instructions per Far Branch ( Far Branches apply upon transition from
        application to operating system,handling interrupts,exceptions) [lower
        number means higher occurrence rate]]
  tma_info_system_kernel_cpi
       [Cycles Per Instruction for the Operating System (OS) Kernel mode]
  tma_info_system_kernel_utilization
       [Fraction of cycles spent in the Operating System (OS) Kernel mode]

Offcore: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_data_sharing
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to data-sharing accesses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]
  tma_info_bottleneck_cache_memory_bandwidth
       [Total pipeline cost of external Memory- or Cache-Bandwidth related
        bottlenecks]
  tma_info_bottleneck_cache_memory_latency
       [Total pipeline cost of external Memory- or Cache-Latency related
        bottlenecks]
  tma_info_bottleneck_memory_data_tlbs
       [Total pipeline cost of Memory Address Translation related bottlenecks
        (data-side TLBs)]
  tma_info_bottleneck_memory_synchronization
       [Total pipeline cost of Memory Synchronization related bottlenecks
        (data transfers and coherency updates across processors)]
  tma_info_bottleneck_other_bottlenecks
       [Total pipeline cost of remaining bottlenecks in the back-end]
  tma_info_memory_core_l3_cache_access_bw_2t
       [Average per-core data access bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_l2mpki_all
       [L2 cache ([RKL+] true) misses per kilo instruction for all request
        types (including speculative)]
  tma_info_memory_l2mpki_rfo
       [Offcore requests (L2 cache miss) per kilo instruction for demand RFOs]
  tma_info_memory_l3_cache_access_bw
       [Average per-thread data access bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_latency_data_l2_mlp
       [Average Parallel L2 cache miss data reads]
  tma_info_memory_latency_load_l2_miss_latency
       [Average Latency for L2 cache miss demand Loads]
  tma_info_memory_latency_load_l2_mlp
       [Average Parallel L2 cache miss demand Loads]
  tma_info_memory_latency_load_l3_miss_latency
       [Average Latency for L3 cache miss demand Loads]
  tma_lock_latency
       [This metric represents fraction of cycles the CPU spent handling cache
        misses due to lock operations]
  tma_mem_bandwidth
       [This metric estimates fraction of cycles where the core's performance
        was likely hurt due to approaching bandwidth limits of external memory
        - DRAM ([SPR-HBM] and/or HBM)]
  tma_mem_latency
       [This metric estimates fraction of cycles where the performance was
        likely hurt due to latency from external memory - DRAM ([SPR-HBM]
        and/or HBM)]
  tma_sq_full
       [This metric measures fraction of cycles where the Super Queue (SQ) was
        full taking into account all request-types and both hardware SMT
        threads (Logical Processors)]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]
  tma_streaming_stores
       [This metric estimates how often CPU was stalled due to Streaming store
        memory accesses; Streaming store optimize out a read request required
        by RFO stores]

PGO: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_frontend_bound
       [This category represents fraction of slots where the processor's
        Frontend undersupplies its Backend]
  tma_info_branches_cond_nt
       [Fraction of branches that are non-taken conditionals]
  tma_info_branches_cond_tk
       [Fraction of branches that are taken conditionals]
  tma_info_inst_mix_bptkbranch
       [Branch instructions per taken branch]
  tma_info_inst_mix_ipcall
       [Instructions per (near) call (lower number means higher occurrence
        rate)]
  tma_info_inst_mix_iptb
       [Instructions per taken branch]

Pipeline: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_branch_instructions
       [This metric represents fraction of slots where the CPU was retiring
        branch instructions]
  tma_info_core_ilp
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per thread (logical-processor)]
  tma_info_pipeline_execute
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per core]
  tma_info_pipeline_ipassist
       [Instructions per a microcode Assist invocation]
  tma_info_pipeline_retire
       [Average number of Uops retired in cycles where at least one uop has
        retired]
  tma_info_thread_clks
       [Per-Logical Processor actual clocks when the Logical Processor is
        active]
  tma_info_thread_cpi
       [Cycles Per Instruction (per Logical Processor)]
  tma_info_thread_execute_per_issue
       [The ratio of Executed- by Issued-Uops]
  tma_info_thread_uoppi
       [Uops Per Instruction]
  tma_memory_operations
       [This metric represents fraction of slots where the CPU was retiring
        memory operations -- uops for memory load or store accesses]
  tma_nop_instructions
       [This metric represents fraction of slots where the CPU was retiring
        NOP (no op) instructions]
  tma_other_light_ops
       [This metric represents the remaining light uops fraction the CPU has
        executed - remaining means not covered by other sibling nodes]

PortsUtil: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_core_ilp
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per thread (logical-processor)]
  tma_info_pipeline_execute
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per core]
  tma_ports_utilization
       [This metric estimates fraction of cycles the CPU performance was
        potentially limited due to Core computation issues (non
        divider-related)]
  tma_ports_utilized_0
       [This metric represents fraction of cycles CPU executed no uops on any
        execution port (Logical Processor cycles since ICL,Physical Core
        cycles otherwise)]
  tma_ports_utilized_1
       [This metric represents fraction of cycles where the CPU executed total
        of 1 uop per cycle on all execution ports (Logical Processor cycles
        since ICL,Physical Core cycles otherwise)]
  tma_ports_utilized_2
       [This metric represents fraction of cycles CPU executed total of 2 uops
        per cycle on all execution ports (Logical Processor cycles since ICL,
        Physical Core cycles otherwise)]
  tma_ports_utilized_3m
       [This metric represents fraction of cycles CPU executed total of 3 or
        more uops per cycle on all execution ports (Logical Processor cycles
        since ICL,Physical Core cycles otherwise)]
  tma_serializing_operation
       [This metric represents fraction of cycles the CPU issue-pipeline was
        stalled due to serializing operations]

Power: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  C10_Pkg_Residency
       [C10 residency percent per package]
  C2_Pkg_Residency
       [C2 residency percent per package]
  C3_Pkg_Residency
       [C3 residency percent per package]
  C6_Core_Residency
       [C6 residency percent per core]
  C6_Pkg_Residency
       [C6 residency percent per package]
  C7_Core_Residency
       [C7 residency percent per core]
  C7_Pkg_Residency
       [C7 residency percent per package]
  C8_Pkg_Residency
       [C8 residency percent per package]
  C9_Pkg_Residency
       [C9 residency percent per package]
  tma_info_core_epc
       [uops Executed per Cycle]
  tma_info_system_core_frequency
       [Measured Average Core Frequency for unhalted processors [GHz]]
  tma_info_system_power_license0_utilization
       [Fraction of Core cycles where the core was running with power-delivery
        for baseline license level 0]
  tma_info_system_power_license1_utilization
       [Fraction of Core cycles where the core was running with power-delivery
        for license level 1]
  tma_info_system_power_license2_utilization
       [Fraction of Core cycles where the core was running with power-delivery
        for license level 2 (introduced in SKX)]
  tma_info_system_turbo_utilization
       [Average Frequency Utilization relative nominal frequency]

Prefetches: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_inst_mix_ipswpf
       [Instructions per Software prefetch instruction (of any type:
        NTA/T0/T1/T2/Prefetch) (lower number means higher occurrence rate)]

Ret: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_bottleneck_branching_overhead
       [Total pipeline cost of instructions used for program control-flow - a
        subset of the Retiring category in TMA]
  tma_info_bottleneck_irregular_overhead
       [Total pipeline cost of irregular execution (e.g]
  tma_info_bottleneck_useful_work
       [Total pipeline cost of "useful operations" - the portion of Retiring
        category not covered by Branching_Overhead nor Irregular_Overhead]
  tma_info_core_coreipc
       [Instructions Per Cycle across hyper-threads (per physical core)]
  tma_info_core_flopc
       [Floating Point Operations Per Cycle]
  tma_info_pipeline_ipassist
       [Instructions per a microcode Assist invocation]
  tma_info_pipeline_retire
       [Average number of Uops retired in cycles where at least one uop has
        retired]
  tma_info_thread_ipc
       [Instructions Per Cycle (per Logical Processor)]
  tma_info_thread_uoppi
       [Uops Per Instruction]

Retire: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_heavy_operations
       [This metric represents fraction of slots where the CPU was retiring
        heavy-weight operations -- instructions that require two or more uops
        or micro-coded sequences]
  tma_info_pipeline_ipassist
       [Instructions per a microcode Assist invocation]
  tma_info_thread_uoppi
       [Uops Per Instruction]
  tma_light_operations
       [This metric represents fraction of slots where the CPU was retiring
        light-weight operations -- instructions that require no more than one
        uop (micro-operation)]

SMT: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_botlnk_l0_core_bound_likely
       [Probability of Core Bound bottleneck hidden by SMT-profiling artifacts]
  tma_info_core_core_clks
       [Core actual clocks when any Logical Processor is active on the
        Physical Core]
  tma_info_core_coreipc
       [Instructions Per Cycle across hyper-threads (per physical core)]
  tma_info_pipeline_execute
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per core]
  tma_info_system_smt_2t_utilization
       [Fraction of cycles where both hardware Logical Processors were active]
  tma_info_thread_slots_utilization
       [Fraction of Physical Core issue-slots utilized by this Logical
        Processor]

Snoop: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_data_sharing
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to data-sharing accesses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]

SoC: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  UNCORE_FREQ
       [Uncore frequency per die [GHZ]]
  tma_info_system_dram_bw_use
       [Average external Memory Bandwidth Use for reads and writes [GB / sec]]
  tma_info_system_mem_parallel_reads
       [Average number of parallel data read requests to external memory]
  tma_info_system_mem_read_latency
       [Average latency of data read request to external memory (in
        nanoseconds)]
  tma_info_system_socket_clks
       [Socket actual clocks when any core is active on that socket]

Summary: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_inst_mix_instructions
       [Total number of retired Instructions]
  tma_info_system_core_frequency
       [Measured Average Core Frequency for unhalted processors [GHz]]
  tma_info_system_cpu_utilization
       [Average CPU Utilization (percentage)]
  tma_info_system_cpus_utilized
       [Average number of utilized CPUs]
  tma_info_thread_ipc
       [Instructions Per Cycle (per Logical Processor)]

TmaL1: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_backend_bound
       [This category represents fraction of slots where no uops are being
        delivered due to a lack of required resources for accepting new uops
        in the Backend]
  tma_bad_speculation
       [This category represents fraction of slots wasted due to incorrect
        speculations]
  tma_frontend_bound
       [This category represents fraction of slots where the processor's
        Frontend undersupplies its Backend]
  tma_info_core_coreipc
       [Instructions Per Cycle across hyper-threads (per physical core)]
  tma_info_inst_mix_instructions
       [Total number of retired Instructions]
  tma_info_thread_slots
       [Total issue-pipeline slots (per-Physical Core till ICL; per-Logical
        Processor ICL onward)]
  tma_info_thread_slots_utilization
       [Fraction of Physical Core issue-slots utilized by this Logical
        Processor]
  tma_retiring
       [This category represents fraction of slots utilized by useful work
        i.e. issued uops that eventually get retired]

TmaL2: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_core_bound
       [This metric represents fraction of slots where Core non-memory issues
        were of a bottleneck]
  tma_fetch_bandwidth
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend bandwidth issues]
  tma_fetch_latency
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend latency issues]
  tma_heavy_operations
       [This metric represents fraction of slots where the CPU was retiring
        heavy-weight operations -- instructions that require two or more uops
        or micro-coded sequences]
  tma_light_operations
       [This metric represents fraction of slots where the CPU was retiring
        light-weight operations -- instructions that require no more than one
        uop (micro-operation)]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]
  tma_memory_bound
       [This metric represents fraction of slots the Memory subsystem within
        the Backend was a bottleneck]

TmaL3mem: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_dram_bound
       [This metric estimates how often the CPU was stalled on accesses to
        external memory (DRAM) by loads]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_l2_bound
       [This metric estimates how often the CPU was stalled due to L2 cache
        accesses by loads]
  tma_l3_bound
       [This metric estimates how often the CPU was stalled due to loads
        accesses to L3 cache or contended with a sibling Core]
  tma_store_bound
       [This metric estimates how often CPU was stalled due to RFO store
        memory accesses; RFO store issue a read-for-ownership request before
        the write]

TopdownL1: [Metrics for top-down breakdown at level 1]
  tma_backend_bound
       [This category represents fraction of slots where no uops are being
        delivered due to a lack of required resources for accepting new uops
        in the Backend]
  tma_bad_speculation
       [This category represents fraction of slots wasted due to incorrect
        speculations]
  tma_frontend_bound
       [This category represents fraction of slots where the processor's
        Frontend undersupplies its Backend]
  tma_retiring
       [This category represents fraction of slots utilized by useful work
        i.e. issued uops that eventually get retired]

TopdownL2: [Metrics for top-down breakdown at level 2]
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_core_bound
       [This metric represents fraction of slots where Core non-memory issues
        were of a bottleneck]
  tma_fetch_bandwidth
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend bandwidth issues]
  tma_fetch_latency
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend latency issues]
  tma_heavy_operations
       [This metric represents fraction of slots where the CPU was retiring
        heavy-weight operations -- instructions that require two or more uops
        or micro-coded sequences]
  tma_light_operations
       [This metric represents fraction of slots where the CPU was retiring
        light-weight operations -- instructions that require no more than one
        uop (micro-operation)]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]
  tma_memory_bound
       [This metric represents fraction of slots the Memory subsystem within
        the Backend was a bottleneck]

TopdownL3: [Metrics for top-down breakdown at level 3]
  tma_branch_instructions
       [This metric represents fraction of slots where the CPU was retiring
        branch instructions]
  tma_branch_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers]
  tma_divider
       [This metric represents fraction of cycles where the Divider unit was
        active]
  tma_dram_bound
       [This metric estimates how often the CPU was stalled on accesses to
        external memory (DRAM) by loads]
  tma_dsb
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to DSB (decoded uop cache) fetch pipeline]
  tma_dsb_switches
       [This metric represents fraction of cycles the CPU was stalled due to
        switches from DSB to MITE pipelines]
  tma_few_uops_instructions
       [This metric represents fraction of slots where the CPU was retiring
        instructions that that are decoder into two or up to ([SNB+] four;
        [ADL+] five) uops]
  tma_fp_arith
       [This metric represents overall arithmetic floating-point (FP)
        operations fraction the CPU has executed (retired)]
  tma_icache_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        instruction cache misses]
  tma_itlb_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        Instruction TLB (ITLB) misses]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_l2_bound
       [This metric estimates how often the CPU was stalled due to L2 cache
        accesses by loads]
  tma_l3_bound
       [This metric estimates how often the CPU was stalled due to loads
        accesses to L3 cache or contended with a sibling Core]
  tma_lcp
       [This metric represents fraction of cycles CPU was stalled due to
        Length Changing Prefixes (LCPs)]
  tma_lsd
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to LSD (Loop Stream Detector) unit]
  tma_memory_operations
       [This metric represents fraction of slots where the CPU was retiring
        memory operations -- uops for memory load or store accesses]
  tma_microcode_sequencer
       [This metric represents fraction of slots the CPU was retiring uops
        fetched by the Microcode Sequencer (MS) unit]
  tma_mite
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to the MITE pipeline (the legacy decode pipeline)]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]
  tma_other_light_ops
       [This metric represents the remaining light uops fraction the CPU has
        executed - remaining means not covered by other sibling nodes]
  tma_other_mispredicts
       [This metric estimates fraction of slots the CPU was stalled due to
        other cases of misprediction (non-retired x86 branches or other types)]
  tma_other_nukes
       [This metric represents fraction of slots the CPU has wasted due to
        Nukes (Machine Clears) not related to memory ordering]
  tma_ports_utilization
       [This metric estimates fraction of cycles the CPU performance was
        potentially limited due to Core computation issues (non
        divider-related)]
  tma_serializing_operation
       [This metric represents fraction of cycles the CPU issue-pipeline was
        stalled due to serializing operations]
  tma_store_bound
       [This metric estimates how often CPU was stalled due to RFO store
        memory accesses; RFO store issue a read-for-ownership request before
        the write]

TopdownL4: [Metrics for top-down breakdown at level 4]
  tma_4k_aliasing
       [This metric estimates how often memory load accesses were aliased by
        preceding stores (in program order) with a 4K address offset]
  tma_assists
       [This metric estimates fraction of slots the CPU retired uops delivered
        by the Microcode_Sequencer as a result of Assists]
  tma_cisc
       [This metric estimates fraction of cycles the CPU retired uops
        originated from CISC (complex instruction set computer) instruction]
  tma_clears_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Machine Clears]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_data_sharing
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to data-sharing accesses]
  tma_decoder0_alone
       [This metric represents fraction of cycles where decoder-0 was the only
        active decoder]
  tma_dtlb_load
       [This metric roughly estimates the fraction of cycles where the Data
        TLB (DTLB) was missed by load accesses]
  tma_dtlb_store
       [This metric roughly estimates the fraction of cycles spent handling
        first-level data TLB store misses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]
  tma_fp_scalar
       [This metric approximates arithmetic floating-point (FP) scalar uops
        fraction the CPU has retired]
  tma_fp_vector
       [This metric approximates arithmetic floating-point (FP) vector uops
        fraction the CPU has retired aggregated across all vector widths]
  tma_l1_hit_latency
       [This metric roughly estimates fraction of cycles with demand load
        accesses that hit the L1 cache]
  tma_l3_hit_latency
       [This metric estimates fraction of cycles with demand load accesses
        that hit the L3 cache under unloaded scenarios (possibly L3 latency
        limited)]
  tma_lock_latency
       [This metric represents fraction of cycles the CPU spent handling cache
        misses due to lock operations]
  tma_mem_bandwidth
       [This metric estimates fraction of cycles where the core's performance
        was likely hurt due to approaching bandwidth limits of external memory
        - DRAM ([SPR-HBM] and/or HBM)]
  tma_mem_latency
       [This metric estimates fraction of cycles where the performance was
        likely hurt due to latency from external memory - DRAM ([SPR-HBM]
        and/or HBM)]
  tma_mispredicts_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Branch Misprediction at execution stage]
  tma_mite_4wide
       [This metric represents fraction of cycles where (only) 4 uops were
        delivered by the MITE pipeline]
  tma_nop_instructions
       [This metric represents fraction of slots where the CPU was retiring
        NOP (no op) instructions]
  tma_ports_utilized_0
       [This metric represents fraction of cycles CPU executed no uops on any
        execution port (Logical Processor cycles since ICL,Physical Core
        cycles otherwise)]
  tma_ports_utilized_1
       [This metric represents fraction of cycles where the CPU executed total
        of 1 uop per cycle on all execution ports (Logical Processor cycles
        since ICL,Physical Core cycles otherwise)]
  tma_ports_utilized_2
       [This metric represents fraction of cycles CPU executed total of 2 uops
        per cycle on all execution ports (Logical Processor cycles since ICL,
        Physical Core cycles otherwise)]
  tma_ports_utilized_3m
       [This metric represents fraction of cycles CPU executed total of 3 or
        more uops per cycle on all execution ports (Logical Processor cycles
        since ICL,Physical Core cycles otherwise)]
  tma_slow_pause
       [This metric represents fraction of cycles the CPU was stalled due to
        PAUSE Instructions]
  tma_split_loads
       [This metric estimates fraction of cycles handling memory load split
        accesses - load that cross 64-byte cache line boundary]
  tma_split_stores
       [This metric represents rate of split store accesses]
  tma_sq_full
       [This metric measures fraction of cycles where the Super Queue (SQ) was
        full taking into account all request-types and both hardware SMT
        threads (Logical Processors)]
  tma_store_fwd_blk
       [This metric roughly estimates fraction of cycles when the memory
        subsystem had loads blocked since they could not forward data from
        earlier (in program order) overlapping stores]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]
  tma_streaming_stores
       [This metric estimates how often CPU was stalled due to Streaming store
        memory accesses; Streaming store optimize out a read request required
        by RFO stores]
  tma_unknown_branches
       [This metric represents fraction of cycles the CPU was stalled due to
        new branch address clears]
  tma_x87_use
       [This metric serves as an approximation of legacy x87 usage]

TopdownL5: [Metrics for top-down breakdown at level 5]
  tma_alu_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution ports for ALU operations]
  tma_fp_assists
       [This metric roughly estimates fraction of slots the CPU retired uops
        as a result of handing Floating Point (FP) Assists]
  tma_fp_vector_128b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 128-bit wide vectors]
  tma_fp_vector_256b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 256-bit wide vectors]
  tma_fp_vector_512b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 512-bit wide vectors]
  tma_load_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port for Load operations]
  tma_load_stlb_hit
       [This metric roughly estimates the fraction of cycles where the (first
        level) DTLB was missed by load accesses,that later on hit in
        second-level TLB (STLB)]
  tma_load_stlb_miss
       [This metric estimates the fraction of cycles where the Second-level
        TLB (STLB) was missed by load accesses,performing a hardware page walk]
  tma_mixing_vectors
       [This metric estimates penalty in terms of percentage of([SKL+]
        injected blend uops out of all Uops Issued -- the Count Domain; [ADL+]
        cycles)]
  tma_store_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port for Store operations]
  tma_store_stlb_hit
       [This metric roughly estimates the fraction of cycles where the TLB was
        missed by store accesses,hitting in the second-level TLB (STLB)]
  tma_store_stlb_miss
       [This metric estimates the fraction of cycles where the STLB was missed
        by store accesses,performing a hardware page walk]

TopdownL6: [Metrics for top-down breakdown at level 6]
  tma_port_0
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 0 ([SNB+] ALU; [HSW+] ALU and 2nd branch)]
  tma_port_1
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 1 (ALU)]
  tma_port_5
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 5 ([SNB+] Branches and ALU; [HSW+] ALU)]
  tma_port_6
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 6 ([HSW+] Primary Branch and simple ALU)]

smi:
  smi_cycles
       [Percentage of cycles spent in System Management Interrupts]
  smi_num
       [Number of SMI interrupts]

tma_L1_group: [Metrics for top-down breakdown at level 1]
  tma_backend_bound
       [This category represents fraction of slots where no uops are being
        delivered due to a lack of required resources for accepting new uops
        in the Backend]
  tma_bad_speculation
       [This category represents fraction of slots wasted due to incorrect
        speculations]
  tma_frontend_bound
       [This category represents fraction of slots where the processor's
        Frontend undersupplies its Backend]
  tma_info_core_coreipc
       [Instructions Per Cycle across hyper-threads (per physical core)]
  tma_info_inst_mix_instructions
       [Total number of retired Instructions]
  tma_info_thread_slots
       [Total issue-pipeline slots (per-Physical Core till ICL; per-Logical
        Processor ICL onward)]
  tma_info_thread_slots_utilization
       [Fraction of Physical Core issue-slots utilized by this Logical
        Processor]
  tma_retiring
       [This category represents fraction of slots utilized by useful work
        i.e. issued uops that eventually get retired]

tma_L2_group: [Metrics for top-down breakdown at level 2]
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_core_bound
       [This metric represents fraction of slots where Core non-memory issues
        were of a bottleneck]
  tma_fetch_bandwidth
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend bandwidth issues]
  tma_fetch_latency
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend latency issues]
  tma_heavy_operations
       [This metric represents fraction of slots where the CPU was retiring
        heavy-weight operations -- instructions that require two or more uops
        or micro-coded sequences]
  tma_light_operations
       [This metric represents fraction of slots where the CPU was retiring
        light-weight operations -- instructions that require no more than one
        uop (micro-operation)]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]
  tma_memory_bound
       [This metric represents fraction of slots the Memory subsystem within
        the Backend was a bottleneck]

tma_L3_group: [Metrics for top-down breakdown at level 3]
  tma_branch_instructions
       [This metric represents fraction of slots where the CPU was retiring
        branch instructions]
  tma_branch_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers]
  tma_divider
       [This metric represents fraction of cycles where the Divider unit was
        active]
  tma_dram_bound
       [This metric estimates how often the CPU was stalled on accesses to
        external memory (DRAM) by loads]
  tma_dsb
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to DSB (decoded uop cache) fetch pipeline]
  tma_dsb_switches
       [This metric represents fraction of cycles the CPU was stalled due to
        switches from DSB to MITE pipelines]
  tma_few_uops_instructions
       [This metric represents fraction of slots where the CPU was retiring
        instructions that that are decoder into two or up to ([SNB+] four;
        [ADL+] five) uops]
  tma_fp_arith
       [This metric represents overall arithmetic floating-point (FP)
        operations fraction the CPU has executed (retired)]
  tma_icache_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        instruction cache misses]
  tma_itlb_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        Instruction TLB (ITLB) misses]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_l2_bound
       [This metric estimates how often the CPU was stalled due to L2 cache
        accesses by loads]
  tma_l3_bound
       [This metric estimates how often the CPU was stalled due to loads
        accesses to L3 cache or contended with a sibling Core]
  tma_lcp
       [This metric represents fraction of cycles CPU was stalled due to
        Length Changing Prefixes (LCPs)]
  tma_lsd
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to LSD (Loop Stream Detector) unit]
  tma_memory_operations
       [This metric represents fraction of slots where the CPU was retiring
        memory operations -- uops for memory load or store accesses]
  tma_microcode_sequencer
       [This metric represents fraction of slots the CPU was retiring uops
        fetched by the Microcode Sequencer (MS) unit]
  tma_mite
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to the MITE pipeline (the legacy decode pipeline)]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]
  tma_other_light_ops
       [This metric represents the remaining light uops fraction the CPU has
        executed - remaining means not covered by other sibling nodes]
  tma_other_mispredicts
       [This metric estimates fraction of slots the CPU was stalled due to
        other cases of misprediction (non-retired x86 branches or other types)]
  tma_other_nukes
       [This metric represents fraction of slots the CPU has wasted due to
        Nukes (Machine Clears) not related to memory ordering]
  tma_ports_utilization
       [This metric estimates fraction of cycles the CPU performance was
        potentially limited due to Core computation issues (non
        divider-related)]
  tma_serializing_operation
       [This metric represents fraction of cycles the CPU issue-pipeline was
        stalled due to serializing operations]
  tma_store_bound
       [This metric estimates how often CPU was stalled due to RFO store
        memory accesses; RFO store issue a read-for-ownership request before
        the write]

tma_L4_group: [Metrics for top-down breakdown at level 4]
  tma_4k_aliasing
       [This metric estimates how often memory load accesses were aliased by
        preceding stores (in program order) with a 4K address offset]
  tma_assists
       [This metric estimates fraction of slots the CPU retired uops delivered
        by the Microcode_Sequencer as a result of Assists]
  tma_cisc
       [This metric estimates fraction of cycles the CPU retired uops
        originated from CISC (complex instruction set computer) instruction]
  tma_clears_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Machine Clears]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_data_sharing
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to data-sharing accesses]
  tma_decoder0_alone
       [This metric represents fraction of cycles where decoder-0 was the only
        active decoder]
  tma_dtlb_load
       [This metric roughly estimates the fraction of cycles where the Data
        TLB (DTLB) was missed by load accesses]
  tma_dtlb_store
       [This metric roughly estimates the fraction of cycles spent handling
        first-level data TLB store misses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]
  tma_fp_scalar
       [This metric approximates arithmetic floating-point (FP) scalar uops
        fraction the CPU has retired]
  tma_fp_vector
       [This metric approximates arithmetic floating-point (FP) vector uops
        fraction the CPU has retired aggregated across all vector widths]
  tma_l1_hit_latency
       [This metric roughly estimates fraction of cycles with demand load
        accesses that hit the L1 cache]
  tma_l3_hit_latency
       [This metric estimates fraction of cycles with demand load accesses
        that hit the L3 cache under unloaded scenarios (possibly L3 latency
        limited)]
  tma_lock_latency
       [This metric represents fraction of cycles the CPU spent handling cache
        misses due to lock operations]
  tma_mem_bandwidth
       [This metric estimates fraction of cycles where the core's performance
        was likely hurt due to approaching bandwidth limits of external memory
        - DRAM ([SPR-HBM] and/or HBM)]
  tma_mem_latency
       [This metric estimates fraction of cycles where the performance was
        likely hurt due to latency from external memory - DRAM ([SPR-HBM]
        and/or HBM)]
  tma_mispredicts_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Branch Misprediction at execution stage]
  tma_mite_4wide
       [This metric represents fraction of cycles where (only) 4 uops were
        delivered by the MITE pipeline]
  tma_nop_instructions
       [This metric represents fraction of slots where the CPU was retiring
        NOP (no op) instructions]
  tma_ports_utilized_0
       [This metric represents fraction of cycles CPU executed no uops on any
        execution port (Logical Processor cycles since ICL,Physical Core
        cycles otherwise)]
  tma_ports_utilized_1
       [This metric represents fraction of cycles where the CPU executed total
        of 1 uop per cycle on all execution ports (Logical Processor cycles
        since ICL,Physical Core cycles otherwise)]
  tma_ports_utilized_2
       [This metric represents fraction of cycles CPU executed total of 2 uops
        per cycle on all execution ports (Logical Processor cycles since ICL,
        Physical Core cycles otherwise)]
  tma_ports_utilized_3m
       [This metric represents fraction of cycles CPU executed total of 3 or
        more uops per cycle on all execution ports (Logical Processor cycles
        since ICL,Physical Core cycles otherwise)]
  tma_slow_pause
       [This metric represents fraction of cycles the CPU was stalled due to
        PAUSE Instructions]
  tma_split_loads
       [This metric estimates fraction of cycles handling memory load split
        accesses - load that cross 64-byte cache line boundary]
  tma_split_stores
       [This metric represents rate of split store accesses]
  tma_sq_full
       [This metric measures fraction of cycles where the Super Queue (SQ) was
        full taking into account all request-types and both hardware SMT
        threads (Logical Processors)]
  tma_store_fwd_blk
       [This metric roughly estimates fraction of cycles when the memory
        subsystem had loads blocked since they could not forward data from
        earlier (in program order) overlapping stores]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]
  tma_streaming_stores
       [This metric estimates how often CPU was stalled due to Streaming store
        memory accesses; Streaming store optimize out a read request required
        by RFO stores]
  tma_unknown_branches
       [This metric represents fraction of cycles the CPU was stalled due to
        new branch address clears]
  tma_x87_use
       [This metric serves as an approximation of legacy x87 usage]

tma_L5_group: [Metrics for top-down breakdown at level 5]
  tma_alu_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution ports for ALU operations]
  tma_fp_assists
       [This metric roughly estimates fraction of slots the CPU retired uops
        as a result of handing Floating Point (FP) Assists]
  tma_fp_vector_128b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 128-bit wide vectors]
  tma_fp_vector_256b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 256-bit wide vectors]
  tma_fp_vector_512b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 512-bit wide vectors]
  tma_load_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port for Load operations]
  tma_load_stlb_hit
       [This metric roughly estimates the fraction of cycles where the (first
        level) DTLB was missed by load accesses,that later on hit in
        second-level TLB (STLB)]
  tma_load_stlb_miss
       [This metric estimates the fraction of cycles where the Second-level
        TLB (STLB) was missed by load accesses,performing a hardware page walk]
  tma_mixing_vectors
       [This metric estimates penalty in terms of percentage of([SKL+]
        injected blend uops out of all Uops Issued -- the Count Domain; [ADL+]
        cycles)]
  tma_store_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port for Store operations]
  tma_store_stlb_hit
       [This metric roughly estimates the fraction of cycles where the TLB was
        missed by store accesses,hitting in the second-level TLB (STLB)]
  tma_store_stlb_miss
       [This metric estimates the fraction of cycles where the STLB was missed
        by store accesses,performing a hardware page walk]

tma_L6_group: [Metrics for top-down breakdown at level 6]
  tma_port_0
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 0 ([SNB+] ALU; [HSW+] ALU and 2nd branch)]
  tma_port_1
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 1 (ALU)]
  tma_port_5
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 5 ([SNB+] Branches and ALU; [HSW+] ALU)]
  tma_port_6
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 6 ([HSW+] Primary Branch and simple ALU)]

tma_alu_op_utilization_group: [Metrics contributing to tma_alu_op_utilization category]
  tma_port_0
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 0 ([SNB+] ALU; [HSW+] ALU and 2nd branch)]
  tma_port_1
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 1 (ALU)]
  tma_port_5
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 5 ([SNB+] Branches and ALU; [HSW+] ALU)]
  tma_port_6
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 6 ([HSW+] Primary Branch and simple ALU)]

tma_assists_group: [Metrics contributing to tma_assists category]
  tma_fp_assists
       [This metric roughly estimates fraction of slots the CPU retired uops
        as a result of handing Floating Point (FP) Assists]

tma_backend_bound_group: [Metrics contributing to tma_backend_bound category]
  tma_core_bound
       [This metric represents fraction of slots where Core non-memory issues
        were of a bottleneck]
  tma_memory_bound
       [This metric represents fraction of slots the Memory subsystem within
        the Backend was a bottleneck]

tma_bad_speculation_group: [Metrics contributing to tma_bad_speculation category]
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]

tma_branch_mispredicts_group: [Metrics contributing to tma_branch_mispredicts category]
  tma_other_mispredicts
       [This metric estimates fraction of slots the CPU was stalled due to
        other cases of misprediction (non-retired x86 branches or other types)]

tma_branch_resteers_group: [Metrics contributing to tma_branch_resteers category]
  tma_clears_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Machine Clears]
  tma_mispredicts_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Branch Misprediction at execution stage]
  tma_unknown_branches
       [This metric represents fraction of cycles the CPU was stalled due to
        new branch address clears]

tma_core_bound_group: [Metrics contributing to tma_core_bound category]
  tma_divider
       [This metric represents fraction of cycles where the Divider unit was
        active]
  tma_ports_utilization
       [This metric estimates fraction of cycles the CPU performance was
        potentially limited due to Core computation issues (non
        divider-related)]
  tma_serializing_operation
       [This metric represents fraction of cycles the CPU issue-pipeline was
        stalled due to serializing operations]

tma_dram_bound_group: [Metrics contributing to tma_dram_bound category]
  tma_mem_bandwidth
       [This metric estimates fraction of cycles where the core's performance
        was likely hurt due to approaching bandwidth limits of external memory
        - DRAM ([SPR-HBM] and/or HBM)]
  tma_mem_latency
       [This metric estimates fraction of cycles where the performance was
        likely hurt due to latency from external memory - DRAM ([SPR-HBM]
        and/or HBM)]

tma_dtlb_load_group: [Metrics contributing to tma_dtlb_load category]
  tma_load_stlb_hit
       [This metric roughly estimates the fraction of cycles where the (first
        level) DTLB was missed by load accesses,that later on hit in
        second-level TLB (STLB)]
  tma_load_stlb_miss
       [This metric estimates the fraction of cycles where the Second-level
        TLB (STLB) was missed by load accesses,performing a hardware page walk]

tma_dtlb_store_group: [Metrics contributing to tma_dtlb_store category]
  tma_store_stlb_hit
       [This metric roughly estimates the fraction of cycles where the TLB was
        missed by store accesses,hitting in the second-level TLB (STLB)]
  tma_store_stlb_miss
       [This metric estimates the fraction of cycles where the STLB was missed
        by store accesses,performing a hardware page walk]

tma_fetch_bandwidth_group: [Metrics contributing to tma_fetch_bandwidth category]
  tma_dsb
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to DSB (decoded uop cache) fetch pipeline]
  tma_lsd
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to LSD (Loop Stream Detector) unit]
  tma_mite
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to the MITE pipeline (the legacy decode pipeline)]

tma_fetch_latency_group: [Metrics contributing to tma_fetch_latency category]
  tma_branch_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers]
  tma_dsb_switches
       [This metric represents fraction of cycles the CPU was stalled due to
        switches from DSB to MITE pipelines]
  tma_icache_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        instruction cache misses]
  tma_itlb_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        Instruction TLB (ITLB) misses]
  tma_lcp
       [This metric represents fraction of cycles CPU was stalled due to
        Length Changing Prefixes (LCPs)]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]

tma_fp_arith_group: [Metrics contributing to tma_fp_arith category]
  tma_fp_scalar
       [This metric approximates arithmetic floating-point (FP) scalar uops
        fraction the CPU has retired]
  tma_fp_vector
       [This metric approximates arithmetic floating-point (FP) vector uops
        fraction the CPU has retired aggregated across all vector widths]
  tma_x87_use
       [This metric serves as an approximation of legacy x87 usage]

tma_fp_vector_group: [Metrics contributing to tma_fp_vector category]
  tma_fp_vector_128b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 128-bit wide vectors]
  tma_fp_vector_256b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 256-bit wide vectors]
  tma_fp_vector_512b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 512-bit wide vectors]

tma_frontend_bound_group: [Metrics contributing to tma_frontend_bound category]
  tma_fetch_bandwidth
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend bandwidth issues]
  tma_fetch_latency
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend latency issues]

tma_heavy_operations_group: [Metrics contributing to tma_heavy_operations category]
  tma_few_uops_instructions
       [This metric represents fraction of slots where the CPU was retiring
        instructions that that are decoder into two or up to ([SNB+] four;
        [ADL+] five) uops]
  tma_microcode_sequencer
       [This metric represents fraction of slots the CPU was retiring uops
        fetched by the Microcode Sequencer (MS) unit]

tma_issue2P: [Metrics related by the issue $issue2P]
  tma_fp_scalar
       [This metric approximates arithmetic floating-point (FP) scalar uops
        fraction the CPU has retired]
  tma_fp_vector
       [This metric approximates arithmetic floating-point (FP) vector uops
        fraction the CPU has retired aggregated across all vector widths]
  tma_fp_vector_128b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 128-bit wide vectors]
  tma_fp_vector_256b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 256-bit wide vectors]
  tma_fp_vector_512b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 512-bit wide vectors]
  tma_port_0
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 0 ([SNB+] ALU; [HSW+] ALU and 2nd branch)]
  tma_port_1
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 1 (ALU)]
  tma_port_5
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 5 ([SNB+] Branches and ALU; [HSW+] ALU)]
  tma_port_6
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 6 ([HSW+] Primary Branch and simple ALU)]
  tma_ports_utilized_2
       [This metric represents fraction of cycles CPU executed total of 2 uops
        per cycle on all execution ports (Logical Processor cycles since ICL,
        Physical Core cycles otherwise)]

tma_issueBM: [Metrics related by the issue $issueBM]
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_info_bad_spec_branch_misprediction_cost
       [Branch Misprediction Cost: Fraction of TMA slots wasted per
        non-speculative branch misprediction (retired JEClear)]
  tma_info_bottleneck_mispredictions
       [Total pipeline cost of Branch Misprediction related bottlenecks]
  tma_mispredicts_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Branch Misprediction at execution stage]

tma_issueBW: [Metrics related by the issue $issueBW]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]
  tma_info_bottleneck_cache_memory_bandwidth
       [Total pipeline cost of external Memory- or Cache-Bandwidth related
        bottlenecks]
  tma_info_system_dram_bw_use
       [Average external Memory Bandwidth Use for reads and writes [GB / sec]]
  tma_mem_bandwidth
       [This metric estimates fraction of cycles where the core's performance
        was likely hurt due to approaching bandwidth limits of external memory
        - DRAM ([SPR-HBM] and/or HBM)]
  tma_sq_full
       [This metric measures fraction of cycles where the Super Queue (SQ) was
        full taking into account all request-types and both hardware SMT
        threads (Logical Processors)]

tma_issueComp: [Metrics related by the issue $issueComp]
  tma_info_bottleneck_compute_bound_est
       [Total pipeline cost when the execution is compute-bound - an
        estimation]

tma_issueD0: [Metrics related by the issue $issueD0]
  tma_decoder0_alone
       [This metric represents fraction of cycles where decoder-0 was the only
        active decoder]
  tma_few_uops_instructions
       [This metric represents fraction of slots where the CPU was retiring
        instructions that that are decoder into two or up to ([SNB+] four;
        [ADL+] five) uops]

tma_issueFB: [Metrics related by the issue $issueFB]
  tma_dsb_switches
       [This metric represents fraction of cycles the CPU was stalled due to
        switches from DSB to MITE pipelines]
  tma_fetch_bandwidth
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend bandwidth issues]
  tma_info_botlnk_l2_dsb_bandwidth
       [Total pipeline cost of DSB (uop cache) hits - subset of the
        Instruction_Fetch_BW Bottleneck]
  tma_info_botlnk_l2_dsb_misses
       [Total pipeline cost of DSB (uop cache) misses - subset of the
        Instruction_Fetch_BW Bottleneck]
  tma_info_frontend_dsb_coverage
       [Fraction of Uops delivered by the DSB (aka Decoded ICache; or Uop
        Cache)]
  tma_info_inst_mix_iptb
       [Instructions per taken branch]
  tma_lcp
       [This metric represents fraction of cycles CPU was stalled due to
        Length Changing Prefixes (LCPs)]

tma_issueFL: [Metrics related by the issue $issueFL]
  tma_info_botlnk_l2_ic_misses
       [Total pipeline cost of Instruction Cache misses - subset of the
        Big_Code Bottleneck]

tma_issueL1: [Metrics related by the issue $issueL1]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_ports_utilized_1
       [This metric represents fraction of cycles where the CPU executed total
        of 1 uop per cycle on all execution ports (Logical Processor cycles
        since ICL,Physical Core cycles otherwise)]

tma_issueLat: [Metrics related by the issue $issueLat]
  tma_info_bottleneck_cache_memory_latency
       [Total pipeline cost of external Memory- or Cache-Latency related
        bottlenecks]
  tma_l3_hit_latency
       [This metric estimates fraction of cycles with demand load accesses
        that hit the L3 cache under unloaded scenarios (possibly L3 latency
        limited)]
  tma_mem_latency
       [This metric estimates fraction of cycles where the performance was
        likely hurt due to latency from external memory - DRAM ([SPR-HBM]
        and/or HBM)]

tma_issueMC: [Metrics related by the issue $issueMC]
  tma_clears_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Machine Clears]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]
  tma_microcode_sequencer
       [This metric represents fraction of slots the CPU was retiring uops
        fetched by the Microcode Sequencer (MS) unit]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]

tma_issueMS: [Metrics related by the issue $issueMS]
  tma_info_bottleneck_irregular_overhead
       [Total pipeline cost of irregular execution (e.g]
  tma_microcode_sequencer
       [This metric represents fraction of slots the CPU was retiring uops
        fetched by the Microcode Sequencer (MS) unit]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]

tma_issueMV: [Metrics related by the issue $issueMV]
  tma_mixing_vectors
       [This metric estimates penalty in terms of percentage of([SKL+]
        injected blend uops out of all Uops Issued -- the Count Domain; [ADL+]
        cycles)]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]

tma_issueRFO: [Metrics related by the issue $issueRFO]
  tma_lock_latency
       [This metric represents fraction of cycles the CPU spent handling cache
        misses due to lock operations]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]

tma_issueSL: [Metrics related by the issue $issueSL]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]

tma_issueSO: [Metrics related by the issue $issueSO]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]
  tma_serializing_operation
       [This metric represents fraction of cycles the CPU issue-pipeline was
        stalled due to serializing operations]

tma_issueSmSt: [Metrics related by the issue $issueSmSt]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]
  tma_streaming_stores
       [This metric estimates how often CPU was stalled due to Streaming store
        memory accesses; Streaming store optimize out a read request required
        by RFO stores]

tma_issueSpSt: [Metrics related by the issue $issueSpSt]
  tma_split_stores
       [This metric represents rate of split store accesses]

tma_issueSyncxn: [Metrics related by the issue $issueSyncxn]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_data_sharing
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to data-sharing accesses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]

tma_issueTLB: [Metrics related by the issue $issueTLB]
  tma_dtlb_load
       [This metric roughly estimates the fraction of cycles where the Data
        TLB (DTLB) was missed by load accesses]
  tma_dtlb_store
       [This metric roughly estimates the fraction of cycles spent handling
        first-level data TLB store misses]
  tma_info_bottleneck_memory_data_tlbs
       [Total pipeline cost of Memory Address Translation related bottlenecks
        (data-side TLBs)]
  tma_info_bottleneck_memory_synchronization
       [Total pipeline cost of Memory Synchronization related bottlenecks
        (data transfers and coherency updates across processors)]

tma_l1_bound_group: [Metrics contributing to tma_l1_bound category]
  tma_4k_aliasing
       [This metric estimates how often memory load accesses were aliased by
        preceding stores (in program order) with a 4K address offset]
  tma_dtlb_load
       [This metric roughly estimates the fraction of cycles where the Data
        TLB (DTLB) was missed by load accesses]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]
  tma_l1_hit_latency
       [This metric roughly estimates fraction of cycles with demand load
        accesses that hit the L1 cache]
  tma_lock_latency
       [This metric represents fraction of cycles the CPU spent handling cache
        misses due to lock operations]
  tma_split_loads
       [This metric estimates fraction of cycles handling memory load split
        accesses - load that cross 64-byte cache line boundary]
  tma_store_fwd_blk
       [This metric roughly estimates fraction of cycles when the memory
        subsystem had loads blocked since they could not forward data from
        earlier (in program order) overlapping stores]

tma_l3_bound_group: [Metrics contributing to tma_l3_bound category]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_data_sharing
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to data-sharing accesses]
  tma_l3_hit_latency
       [This metric estimates fraction of cycles with demand load accesses
        that hit the L3 cache under unloaded scenarios (possibly L3 latency
        limited)]
  tma_sq_full
       [This metric measures fraction of cycles where the Super Queue (SQ) was
        full taking into account all request-types and both hardware SMT
        threads (Logical Processors)]

tma_light_operations_group: [Metrics contributing to tma_light_operations category]
  tma_branch_instructions
       [This metric represents fraction of slots where the CPU was retiring
        branch instructions]
  tma_fp_arith
       [This metric represents overall arithmetic floating-point (FP)
        operations fraction the CPU has executed (retired)]
  tma_memory_operations
       [This metric represents fraction of slots where the CPU was retiring
        memory operations -- uops for memory load or store accesses]
  tma_other_light_ops
       [This metric represents the remaining light uops fraction the CPU has
        executed - remaining means not covered by other sibling nodes]

tma_machine_clears_group: [Metrics contributing to tma_machine_clears category]
  tma_other_nukes
       [This metric represents fraction of slots the CPU has wasted due to
        Nukes (Machine Clears) not related to memory ordering]

tma_memory_bound_group: [Metrics contributing to tma_memory_bound category]
  tma_dram_bound
       [This metric estimates how often the CPU was stalled on accesses to
        external memory (DRAM) by loads]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_l2_bound
       [This metric estimates how often the CPU was stalled due to L2 cache
        accesses by loads]
  tma_l3_bound
       [This metric estimates how often the CPU was stalled due to loads
        accesses to L3 cache or contended with a sibling Core]
  tma_store_bound
       [This metric estimates how often CPU was stalled due to RFO store
        memory accesses; RFO store issue a read-for-ownership request before
        the write]

tma_microcode_sequencer_group: [Metrics contributing to tma_microcode_sequencer category]
  tma_assists
       [This metric estimates fraction of slots the CPU retired uops delivered
        by the Microcode_Sequencer as a result of Assists]
  tma_cisc
       [This metric estimates fraction of cycles the CPU retired uops
        originated from CISC (complex instruction set computer) instruction]

tma_mite_group: [Metrics contributing to tma_mite category]
  tma_decoder0_alone
       [This metric represents fraction of cycles where decoder-0 was the only
        active decoder]
  tma_mite_4wide
       [This metric represents fraction of cycles where (only) 4 uops were
        delivered by the MITE pipeline]

tma_other_light_ops_group: [Metrics contributing to tma_other_light_ops category]
  tma_nop_instructions
       [This metric represents fraction of slots where the CPU was retiring
        NOP (no op) instructions]

tma_ports_utilization_group: [Metrics contributing to tma_ports_utilization category]
  tma_ports_utilized_0
       [This metric represents fraction of cycles CPU executed no uops on any
        execution port (Logical Processor cycles since ICL,Physical Core
        cycles otherwise)]
  tma_ports_utilized_1
       [This metric represents fraction of cycles where the CPU executed total
        of 1 uop per cycle on all execution ports (Logical Processor cycles
        since ICL,Physical Core cycles otherwise)]
  tma_ports_utilized_2
       [This metric represents fraction of cycles CPU executed total of 2 uops
        per cycle on all execution ports (Logical Processor cycles since ICL,
        Physical Core cycles otherwise)]
  tma_ports_utilized_3m
       [This metric represents fraction of cycles CPU executed total of 3 or
        more uops per cycle on all execution ports (Logical Processor cycles
        since ICL,Physical Core cycles otherwise)]

tma_ports_utilized_0_group: [Metrics contributing to tma_ports_utilized_0 category]
  tma_mixing_vectors
       [This metric estimates penalty in terms of percentage of([SKL+]
        injected blend uops out of all Uops Issued -- the Count Domain; [ADL+]
        cycles)]

tma_ports_utilized_3m_group: [Metrics contributing to tma_ports_utilized_3m category]
  tma_alu_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution ports for ALU operations]
  tma_load_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port for Load operations]
  tma_store_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port for Store operations]

tma_retiring_group: [Metrics contributing to tma_retiring category]
  tma_heavy_operations
       [This metric represents fraction of slots where the CPU was retiring
        heavy-weight operations -- instructions that require two or more uops
        or micro-coded sequences]
  tma_light_operations
       [This metric represents fraction of slots where the CPU was retiring
        light-weight operations -- instructions that require no more than one
        uop (micro-operation)]

tma_serializing_operation_group: [Metrics contributing to tma_serializing_operation category]
  tma_slow_pause
       [This metric represents fraction of cycles the CPU was stalled due to
        PAUSE Instructions]

tma_store_bound_group: [Metrics contributing to tma_store_bound category]
  tma_dtlb_store
       [This metric roughly estimates the fraction of cycles spent handling
        first-level data TLB store misses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]
  tma_split_stores
       [This metric represents rate of split store accesses]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]
  tma_streaming_stores
       [This metric estimates how often CPU was stalled due to Streaming store
        memory accesses; Streaming store optimize out a read request required
        by RFO stores]

transaction:
  tsx_aborted_cycles
       [Percentage of cycles in aborted transactions]
  tsx_cycles_per_elision
       [Number of cycles within a transaction divided by the number of
        elisions]
  tsx_cycles_per_transaction
       [Number of cycles within a transaction divided by the number of
        transactions]
  tsx_transactional_cycles
       [Percentage of cycles within a transaction region]
